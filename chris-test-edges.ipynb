{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m64 packages\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m25 packages\u001b[0m \u001b[2min 21.38s\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1masttokens\u001b[0m\u001b[2m==3.0.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mcomm\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mdebugpy\u001b[0m\u001b[2m==1.8.17\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mdecorator\u001b[0m\u001b[2m==5.2.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mexecuting\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==9.7.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipython-pygments-lexers\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.6.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjupyter-core\u001b[0m\u001b[2m==5.9.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mmatplotlib-inline\u001b[0m\u001b[2m==0.2.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnest-asyncio\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mparso\u001b[0m\u001b[2m==0.8.5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpexpect\u001b[0m\u001b[2m==4.9.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.5.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.52\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mptyprocess\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpure-eval\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.1.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mstack-data\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtornado\u001b[0m\u001b[2m==6.5.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtraitlets\u001b[0m\u001b[2m==5.14.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mwcwidth\u001b[0m\u001b[2m==0.2.14\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment ===\n",
      "Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39\n",
      "Python: 3.12.3\n",
      "\n",
      "=== PyTorch / CUDA Info ===\n",
      "torch.__version__: 2.8.0+cu128\n",
      "torch.version.cuda: 12.8\n",
      "torch.cuda.is_available(): True\n",
      "torch.cuda.device_count(): 1\n",
      "  device 0: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "\n",
      "Successfully ran a matrix multiply on CUDA.\n",
      "z.device: cuda:0\n",
      "\n",
      "=== Test Complete ===\n"
     ]
    }
   ],
   "source": [
    "# save as test_cuda.py and run: python3 test_cuda.py\n",
    "\n",
    "import platform\n",
    "\n",
    "print(\"=== Environment ===\")\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Python:\", platform.python_version())\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except ImportError as e:\n",
    "    print(\"\\nPyTorch is not installed or not in this Python environment.\")\n",
    "    raise SystemExit(e)\n",
    "\n",
    "print(\"\\n=== PyTorch / CUDA Info ===\")\n",
    "print(\"torch.__version__:\", torch.__version__)\n",
    "print(\"torch.version.cuda:\", torch.version.cuda)\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"torch.cuda.is_available():\", cuda_available)\n",
    "\n",
    "if not cuda_available:\n",
    "    print(\"\\nCUDA is NOT available to PyTorch in this environment.\")\n",
    "else:\n",
    "    # Number of devices\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(\"torch.cuda.device_count():\", device_count)\n",
    "\n",
    "    for i in range(device_count):\n",
    "        print(f\"  device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "    # Simple tensor test on GPU\n",
    "    try:\n",
    "        x = torch.rand(3, 3, device=\"cuda\")\n",
    "        y = torch.rand(3, 3, device=\"cuda\")\n",
    "        z = x @ y\n",
    "        print(\"\\nSuccessfully ran a matrix multiply on CUDA.\")\n",
    "        print(\"z.device:\", z.device)\n",
    "    except Exception as e:\n",
    "        print(\"\\nERROR: Allocation or compute on CUDA failed:\")\n",
    "        print(e)\n",
    "\n",
    "print(\"\\n=== Test Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu128\n",
      "12.8\n",
      "91002\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 189ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install pyg-lib -f https://data.pyg.org/whl/torch-2.8.0+cu128.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic: Check `pyg-lib` availability\n",
    "Run the next cell in this notebook kernel to verify whether `pyg_lib` (or `torch_sparse`) is importable here and whether `LinkNeighborLoader` works. If it fails, compare the printed `sys.executable` to your terminal environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Kernel executable ---\n",
      "/mnt/d/SFSU/CSC871/csc871-anti-money-laundering-ibm-gnn/.venv/bin/python\n",
      "\n",
      "--- sys.path (first 6 entries) ---\n",
      "  /usr/lib/python312.zip\n",
      "  /usr/lib/python3.12\n",
      "  /usr/lib/python3.12/lib-dynload\n",
      "  \n",
      "  /mnt/d/SFSU/CSC871/csc871-anti-money-laundering-ibm-gnn/.venv/lib/python3.12/site-packages\n",
      "find_spec('pyg_lib') -> ModuleSpec(name='pyg_lib', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7623ef33bc20>, origin='/mnt/d/SFSU/CSC871/csc871-anti-money-laundering-ibm-gnn/.venv/lib/python3.12/site-packages/pyg_lib/__init__.py', submodule_search_locations=['/mnt/d/SFSU/CSC871/csc871-anti-money-laundering-ibm-gnn/.venv/lib/python3.12/site-packages/pyg_lib'])\n",
      "  origin=/mnt/d/SFSU/CSC871/csc871-anti-money-laundering-ibm-gnn/.venv/lib/python3.12/site-packages/pyg_lib/__init__.py loader=<_frozen_importlib_external.SourceFileLoader object at 0x7623ef33bc20>\n",
      "find_spec('torch_sparse') -> None\n",
      "find_spec('torch_geometric') -> ModuleSpec(name='torch_geometric', loader=<_frozen_importlib_external.SourceFileLoader object at 0x762410c176e0>, origin='/mnt/d/SFSU/CSC871/csc871-anti-money-laundering-ibm-gnn/.venv/lib/python3.12/site-packages/torch_geometric/__init__.py', submodule_search_locations=['/mnt/d/SFSU/CSC871/csc871-anti-money-laundering-ibm-gnn/.venv/lib/python3.12/site-packages/torch_geometric'])\n",
      "  origin=/mnt/d/SFSU/CSC871/csc871-anti-money-laundering-ibm-gnn/.venv/lib/python3.12/site-packages/torch_geometric/__init__.py loader=<_frozen_importlib_external.SourceFileLoader object at 0x762410c176e0>\n",
      "\n",
      "--- Import attempts ---\n",
      "torch.__version__: 2.8.0+cu128\n",
      "torch.version.cuda: 12.8\n",
      "torch.cuda.is_available(): True\n",
      "\n",
      "Imported pyg_lib; version / repr -> 0.5.0+pt28cu128\n",
      "\n",
      "torch_sparse import failed:\n",
      " Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_28090/2240904335.py\", line 39, in <module>\n",
      "    import torch_sparse\n",
      "ModuleNotFoundError: No module named 'torch_sparse'\n",
      "\n",
      "\n",
      "--- Try a minimal LinkNeighborLoader run (if torch_geometric available) ---\n",
      "torch.__version__: 2.8.0+cu128\n",
      "torch.version.cuda: 12.8\n",
      "torch.cuda.is_available(): True\n",
      "\n",
      "Imported pyg_lib; version / repr -> 0.5.0+pt28cu128\n",
      "\n",
      "torch_sparse import failed:\n",
      " Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_28090/2240904335.py\", line 39, in <module>\n",
      "    import torch_sparse\n",
      "ModuleNotFoundError: No module named 'torch_sparse'\n",
      "\n",
      "\n",
      "--- Try a minimal LinkNeighborLoader run (if torch_geometric available) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/SFSU/CSC871/csc871-anti-money-laundering-ibm-gnn/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed tiny Data: Data(x=[3, 4], edge_index=[2, 2], edge_attr=[2, 3])\n",
      "LinkNeighborLoader created successfully. Iterating one batch...\n",
      "  Batch keys: ['num_sampled_edges', 'x', 'n_id', 'num_sampled_nodes', 'edge_label', 'edge_attr', 'edge_label_index', 'edge_index', 'input_id', 'e_id']\n",
      "  batch.edge_label -> tensor([0])\n",
      "\n",
      "--- Diagnostic complete ---\n"
     ]
    }
   ],
   "source": [
    "import sys, importlib, traceback\n",
    "print('--- Kernel executable ---')\n",
    "print(sys.executable)\n",
    "print('\\n--- sys.path (first 6 entries) ---')\n",
    "for p in sys.path[:6]:\n",
    "    print(' ', p)\n",
    "\n",
    "def try_find(name):\n",
    "    try:\n",
    "        spec = importlib.util.find_spec(name)\n",
    "        print(f\"find_spec('{name}') ->\", spec)\n",
    "        if spec is not None:\n",
    "            origin = getattr(spec, 'origin', None)\n",
    "            loader = getattr(spec, 'loader', None)\n",
    "            print(f\"  origin={origin} loader={loader}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while find_spec('{name}'):\", e)\n",
    "\n",
    "try_find('pyg_lib')\n",
    "try_find('torch_sparse')\n",
    "try_find('torch_geometric')\n",
    "\n",
    "print('\\n--- Import attempts ---')\n",
    "try:\n",
    "    import torch\n",
    "    print('torch.__version__:', torch.__version__)\n",
    "    print('torch.version.cuda:', torch.version.cuda)\n",
    "    print('torch.cuda.is_available():', torch.cuda.is_available())\n",
    "except Exception:\n",
    "    print('Failed to import torch:\\n', traceback.format_exc())\n",
    "\n",
    "try:\n",
    "    import pyg_lib\n",
    "    print('\\nImported pyg_lib; version / repr ->', getattr(pyg_lib, '__version__', repr(pyg_lib)))\n",
    "except Exception:\n",
    "    print('\\npyg_lib import failed:\\n', traceback.format_exc())\n",
    "\n",
    "try:\n",
    "    import torch_sparse\n",
    "    print('\\nImported torch_sparse; version / repr ->', getattr(torch_sparse, '__version__', repr(torch_sparse)))\n",
    "except Exception:\n",
    "    print('\\ntorch_sparse import failed:\\n', traceback.format_exc())\n",
    "\n",
    "print('\\n--- Try a minimal LinkNeighborLoader run (if torch_geometric available) ---')\n",
    "try:\n",
    "    from torch_geometric.data import Data\n",
    "    from torch_geometric.loader import LinkNeighborLoader\n",
    "    import torch\n",
    "\n",
    "    edge_index = torch.tensor([[0, 1], [1, 2]], dtype=torch.long)\n",
    "    num_nodes = 3\n",
    "    x = torch.randn((num_nodes, 4))\n",
    "    edge_attr = torch.randn((edge_index.size(1), 3))\n",
    "    edge_label_index = edge_index\n",
    "    edge_label = torch.tensor([0, 1], dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    print('Constructed tiny Data:', data)\n",
    "\n",
    "    try:\n",
    "        loader = LinkNeighborLoader(\n",
    "            data,\n",
    "            num_neighbors=[2, 2],\n",
    "            batch_size=1,\n",
    "            edge_label_index=edge_label_index,\n",
    "            edge_label=edge_label,\n",
    "            shuffle=False,\n",
    "            neg_sampling_ratio=0.0,\n",
    "        )\n",
    "        print('LinkNeighborLoader created successfully. Iterating one batch...')\n",
    "        for b_idx, batch in enumerate(loader):\n",
    "            print('  Batch keys:', list(batch.keys()))\n",
    "            if hasattr(batch, 'edge_label'):\n",
    "                print('  batch.edge_label ->', batch.edge_label)\n",
    "            else:\n",
    "                print('  batch has no edge_label attribute')\n",
    "            break\n",
    "    except Exception:\n",
    "        print('LinkNeighborLoader construction/iteration failed:\\n', traceback.format_exc())\n",
    "\n",
    "except Exception:\n",
    "    print('torch_geometric import or LinkNeighborLoader not available:\\n', traceback.format_exc())\n",
    "\n",
    "print('\\n--- Diagnostic complete ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5078345 rows; columns: ['Timestamp', 'From Bank', 'Account', 'To Bank', 'Account.1', 'Amount Received', 'Receiving Currency', 'Amount Paid', 'Payment Currency', 'Payment Format', 'Is Laundering']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>From Bank</th>\n",
       "      <th>Account</th>\n",
       "      <th>To Bank</th>\n",
       "      <th>Account.1</th>\n",
       "      <th>Amount Received</th>\n",
       "      <th>Receiving Currency</th>\n",
       "      <th>Amount Paid</th>\n",
       "      <th>Payment Currency</th>\n",
       "      <th>Payment Format</th>\n",
       "      <th>Is Laundering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022/09/01 00:20</td>\n",
       "      <td>10</td>\n",
       "      <td>8000EBD30</td>\n",
       "      <td>10</td>\n",
       "      <td>8000EBD30</td>\n",
       "      <td>3697.34</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>3697.34</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022/09/01 00:20</td>\n",
       "      <td>3208</td>\n",
       "      <td>8000F4580</td>\n",
       "      <td>1</td>\n",
       "      <td>8000F5340</td>\n",
       "      <td>0.01</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>0.01</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Cheque</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022/09/01 00:00</td>\n",
       "      <td>3209</td>\n",
       "      <td>8000F4670</td>\n",
       "      <td>3209</td>\n",
       "      <td>8000F4670</td>\n",
       "      <td>14675.57</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>14675.57</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022/09/01 00:02</td>\n",
       "      <td>12</td>\n",
       "      <td>8000F5030</td>\n",
       "      <td>12</td>\n",
       "      <td>8000F5030</td>\n",
       "      <td>2806.97</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>2806.97</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022/09/01 00:06</td>\n",
       "      <td>10</td>\n",
       "      <td>8000F5200</td>\n",
       "      <td>10</td>\n",
       "      <td>8000F5200</td>\n",
       "      <td>36682.97</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>36682.97</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Timestamp  From Bank    Account  To Bank  Account.1  \\\n",
       "0  2022/09/01 00:20         10  8000EBD30       10  8000EBD30   \n",
       "1  2022/09/01 00:20       3208  8000F4580        1  8000F5340   \n",
       "2  2022/09/01 00:00       3209  8000F4670     3209  8000F4670   \n",
       "3  2022/09/01 00:02         12  8000F5030       12  8000F5030   \n",
       "4  2022/09/01 00:06         10  8000F5200       10  8000F5200   \n",
       "\n",
       "   Amount Received Receiving Currency  Amount Paid Payment Currency  \\\n",
       "0          3697.34          US Dollar      3697.34        US Dollar   \n",
       "1             0.01          US Dollar         0.01        US Dollar   \n",
       "2         14675.57          US Dollar     14675.57        US Dollar   \n",
       "3          2806.97          US Dollar      2806.97        US Dollar   \n",
       "4         36682.97          US Dollar     36682.97        US Dollar   \n",
       "\n",
       "  Payment Format  Is Laundering  \n",
       "0   Reinvestment              0  \n",
       "1         Cheque              0  \n",
       "2   Reinvestment              0  \n",
       "3   Reinvestment              0  \n",
       "4   Reinvestment              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the small transactions CSV (relative to this notebook).\n",
    "DATA_PATH = Path(\"dataset\") / \"HI-Small_Trans.csv\"\n",
    "\n",
    "# Load into a DataFrame\n",
    "small_trans = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Quick summary and preview\n",
    "print(f\"Loaded {len(small_trans)} rows; columns: {list(small_trans.columns)}\")\n",
    "small_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset loaded: 5078345 edges\n",
      "Fraud edges: 5177, Non-fraud edges: 5073168, base rate 0.001019\n"
     ]
    }
   ],
   "source": [
    "# Use the full transaction table for modeling and keep its imbalance statistics\n",
    "import numpy as np\n",
    "\n",
    "LABEL_COL = 'Is Laundering'\n",
    "RANDOM_SEED = 17\n",
    "\n",
    "working_trans = small_trans.copy().reset_index(drop=True)\n",
    "\n",
    "pos_count = int(working_trans[LABEL_COL].sum())\n",
    "neg_count = len(working_trans) - pos_count\n",
    "fraud_ratio = pos_count / max(len(working_trans), 1)\n",
    "print(f'Full dataset loaded: {len(working_trans)} edges')\n",
    "print(f'Fraud edges: {pos_count}, Non-fraud edges: {neg_count}, base rate {fraud_ratio:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalance Visualization Overview (IBM AML Dataset)\n",
    "\n",
    "This section adds visual summaries of the strong class imbalance (fraud vs nonâ€‘fraud) and related distributions. Run in order after the dataset has been loaded into `small_trans`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "OVERALL LABEL DISTRIBUTION\n",
      "========================================================================\n",
      "Non-fraud (0)  : 5073168 (99.898%)\n",
      "Fraud (1)      :    5177 ( 0.102%)\n",
      "Fraud ratio overall: 0.00102\n",
      "\n",
      "========================================================================\n",
      "NUMERIC AMOUNT SUMMARY PER CLASS\n",
      "========================================================================\n",
      "Column: Amount Received\n",
      "                 count          mean   median           std     min           max\n",
      "Is Laundering                                                                    \n",
      "0              5073168  5.957962e+06  1407.51  1.036563e+09  0.0000  1.046302e+12\n",
      "1                 5177  3.613531e+07  8667.21  1.527919e+09  0.0032  8.485314e+10\n",
      "----------------------------------------\n",
      "Column: Amount Paid\n",
      "                 count          mean   median           std     min           max\n",
      "Is Laundering                                                                    \n",
      "0              5073168  4.477000e+06  1410.99  8.688463e+08  0.0000  1.046302e+12\n",
      "1                 5177  3.613531e+07  8667.21  1.527919e+09  0.0032  8.485314e+10\n",
      "----------------------------------------\n",
      "\n",
      "========================================================================\n",
      "TEMPORAL FRAUD RATES (first 10 windows)\n",
      "========================================================================\n",
      "Column: Amount Received\n",
      "                 count          mean   median           std     min           max\n",
      "Is Laundering                                                                    \n",
      "0              5073168  5.957962e+06  1407.51  1.036563e+09  0.0000  1.046302e+12\n",
      "1                 5177  3.613531e+07  8667.21  1.527919e+09  0.0032  8.485314e+10\n",
      "----------------------------------------\n",
      "Column: Amount Paid\n",
      "                 count          mean   median           std     min           max\n",
      "Is Laundering                                                                    \n",
      "0              5073168  4.477000e+06  1410.99  8.688463e+08  0.0000  1.046302e+12\n",
      "1                 5177  3.613531e+07  8667.21  1.527919e+09  0.0032  8.485314e+10\n",
      "----------------------------------------\n",
      "\n",
      "========================================================================\n",
      "TEMPORAL FRAUD RATES (first 10 windows)\n",
      "========================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28090/451852787.py:48: FutureWarning: DataFrameGroupBy.resample operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  counts = temp_df.set_index('ts').groupby('label').resample(freq).size().unstack(0).fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using frequency: D\n",
      "            label_0  label_1    total  fraud_rate\n",
      "ts                                               \n",
      "2022-09-01  1114599      322  1114921      0.0003\n",
      "2022-09-02   754041      408   754449      0.0005\n",
      "2022-09-03   206991      391   207382      0.0019\n",
      "2022-09-04   207023      407   207430      0.0020\n",
      "2022-09-05   482179      471   482650      0.0010\n",
      "2022-09-06   481558      531   482089      0.0011\n",
      "2022-09-07   482254      497   482751      0.0010\n",
      "2022-09-08   482234      539   482773      0.0011\n",
      "2022-09-09   653953      514   654467      0.0008\n",
      "2022-09-10   207883      442   208325      0.0021\n",
      "\n",
      "========================================================================\n",
      "ACCOUNT PARTICIPATION SNAPSHOT (top 10)\n",
      "========================================================================\n",
      "Top senders by volume (From Bank):\n",
      "From Bank\n",
      "70     449859\n",
      "10      81629\n",
      "12      79754\n",
      "1       62211\n",
      "15      52511\n",
      "220     52417\n",
      "20      41008\n",
      "3       38413\n",
      "7       31086\n",
      "211     30451\n",
      "\n",
      "Top receivers by volume (To Bank):\n",
      "To Bank\n",
      "10     42547\n",
      "12     41872\n",
      "15     38721\n",
      "220    30625\n",
      "1      30115\n",
      "3      25627\n",
      "7      23029\n",
      "20     22048\n",
      "28     21160\n",
      "211    20576\n",
      "\n",
      "Top senders by fraud count:\n",
      "From Bank\n",
      "70     633\n",
      "12      76\n",
      "20      67\n",
      "119     59\n",
      "10      51\n",
      "1       50\n",
      "11      47\n",
      "15      46\n",
      "22      40\n",
      "118     36\n",
      "\n",
      "Top receivers by fraud count:\n",
      "To Bank\n",
      "12     89\n",
      "119    73\n",
      "11     68\n",
      "20     54\n",
      "1      53\n",
      "10     51\n",
      "22     48\n",
      "222    47\n",
      "23     43\n",
      "15     42\n",
      "\n",
      "\n",
      "========================================================================\n",
      "NUMERIC FEATURE CORRELATIONS WITH FRAUD (top 15 abs(r))\n",
      "========================================================================\n",
      "Top senders by volume (From Bank):\n",
      "From Bank\n",
      "70     449859\n",
      "10      81629\n",
      "12      79754\n",
      "1       62211\n",
      "15      52511\n",
      "220     52417\n",
      "20      41008\n",
      "3       38413\n",
      "7       31086\n",
      "211     30451\n",
      "\n",
      "Top receivers by volume (To Bank):\n",
      "To Bank\n",
      "10     42547\n",
      "12     41872\n",
      "15     38721\n",
      "220    30625\n",
      "1      30115\n",
      "3      25627\n",
      "7      23029\n",
      "20     22048\n",
      "28     21160\n",
      "211    20576\n",
      "\n",
      "Top senders by fraud count:\n",
      "From Bank\n",
      "70     633\n",
      "12      76\n",
      "20      67\n",
      "119     59\n",
      "10      51\n",
      "1       50\n",
      "11      47\n",
      "15      46\n",
      "22      40\n",
      "118     36\n",
      "\n",
      "Top receivers by fraud count:\n",
      "To Bank\n",
      "12     89\n",
      "119    73\n",
      "11     68\n",
      "20     54\n",
      "1      53\n",
      "10     51\n",
      "22     48\n",
      "222    47\n",
      "23     43\n",
      "15     42\n",
      "\n",
      "\n",
      "========================================================================\n",
      "NUMERIC FEATURE CORRELATIONS WITH FRAUD (top 15 abs(r))\n",
      "========================================================================\n",
      "        feature        r  p_value\n",
      "        To Bank -0.00572  0.00000\n",
      "    Amount Paid  0.00116  0.00886\n",
      "Amount Received  0.00093  0.03640\n",
      "      From Bank -0.00023  0.60350\n",
      "        feature        r  p_value\n",
      "        To Bank -0.00572  0.00000\n",
      "    Amount Paid  0.00116  0.00886\n",
      "Amount Received  0.00093  0.03640\n",
      "      From Bank -0.00023  0.60350\n"
     ]
    }
   ],
   "source": [
    "# Text-based imbalance summary for IBM AML dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "assert 'working_trans' in globals(), \"Run the balanced sampling cell to create 'working_trans'.\"\n",
    "df = working_trans.copy()\n",
    "label_col = 'Is Laundering'\n",
    "if label_col not in df.columns:\n",
    "    raise KeyError(f\"Expected column '{label_col}' in the dataset.\")\n",
    "\n",
    "print('=' * 72)\n",
    "print('OVERALL LABEL DISTRIBUTION')\n",
    "print('=' * 72)\n",
    "label_counts = df[label_col].value_counts().sort_index()\n",
    "total = label_counts.sum()\n",
    "for label, count in label_counts.items():\n",
    "    pct = 100.0 * count / total\n",
    "    label_name = 'Fraud (1)' if label == 1 else 'Non-fraud (0)'\n",
    "    print(f\"{label_name:<15}: {count:>7} ({pct:6.3f}%)\")\n",
    "fraud_ratio = label_counts.get(1, 0) / max(total, 1)\n",
    "print(f\"Fraud ratio overall: {fraud_ratio:.5f}\")\n",
    "\n",
    "print('\\n' + '=' * 72)\n",
    "print('NUMERIC AMOUNT SUMMARY PER CLASS')\n",
    "print('=' * 72)\n",
    "amount_cols = [c for c in df.columns if any(k in c.lower() for k in ('amount', 'amt', 'value'))]\n",
    "if amount_cols:\n",
    "    for col in amount_cols:\n",
    "        series = pd.to_numeric(df[col], errors='coerce')\n",
    "        summary = df.groupby(label_col)[col].agg(['count', 'mean', 'median', 'std', 'min', 'max'])\n",
    "        print(f\"Column: {col}\")\n",
    "        print(summary.fillna(0).round(4).to_string())\n",
    "        print('-' * 40)\n",
    "else:\n",
    "    print('No amount-like columns detected for summary.')\n",
    "\n",
    "print('\\n' + '=' * 72)\n",
    "print('TEMPORAL FRAUD RATES (first 10 windows)')\n",
    "print('=' * 72)\n",
    "time_col = next((c for c in df.columns if any(k in c.lower() for k in ('time', 'date', 'timestamp'))), None)\n",
    "if time_col:\n",
    "    ts = pd.to_datetime(df[time_col], errors='coerce')\n",
    "    temp_df = pd.DataFrame({'ts': ts, 'label': df[label_col]}).dropna(subset=['ts'])\n",
    "    span_days = (temp_df['ts'].max() - temp_df['ts'].min()).days\n",
    "    freq = 'D' if span_days >= 2 else 'H'\n",
    "    counts = temp_df.set_index('ts').groupby('label').resample(freq).size().unstack(0).fillna(0)\n",
    "    counts.columns = [f'label_{c}' for c in counts.columns]\n",
    "    counts['total'] = counts.sum(axis=1)\n",
    "    counts['fraud_rate'] = counts.get('label_1', 0) / counts['total'].replace(0, np.nan)\n",
    "    print(f\"Using frequency: {freq}\")\n",
    "    preview = counts[['label_0', 'label_1', 'total', 'fraud_rate']].head(10).fillna(0)\n",
    "    print(preview.round({'fraud_rate': 4}).to_string())\n",
    "else:\n",
    "    print('No timestamp/date column detected for temporal summary.')\n",
    "\n",
    "print('\\n' + '=' * 72)\n",
    "print('ACCOUNT PARTICIPATION SNAPSHOT (top 10)')\n",
    "print('=' * 72)\n",
    "sender_col = next((c for c in df.columns if any(k in c.lower() for k in ('sender', 'originator', 'from', 'account'))), None)\n",
    "receiver_col = next((c for c in df.columns if any(k in c.lower() for k in ('receiver', 'beneficiary', 'to', 'account.1', 'account_1'))), None)\n",
    "if sender_col and receiver_col:\n",
    "    part_df = df[[sender_col, receiver_col, label_col]].copy()\n",
    "    top_senders = part_df.groupby(sender_col).size().sort_values(ascending=False).head(10)\n",
    "    top_receivers = part_df.groupby(receiver_col).size().sort_values(ascending=False).head(10)\n",
    "    fraud_senders = part_df.groupby(sender_col)[label_col].sum().sort_values(ascending=False).head(10)\n",
    "    fraud_receivers = part_df.groupby(receiver_col)[label_col].sum().sort_values(ascending=False).head(10)\n",
    "    print(f\"Top senders by volume ({sender_col}):\\n{top_senders.to_string()}\\n\")\n",
    "    print(f\"Top receivers by volume ({receiver_col}):\\n{top_receivers.to_string()}\\n\")\n",
    "    print(f\"Top senders by fraud count:\\n{fraud_senders.to_string()}\\n\")\n",
    "    print(f\"Top receivers by fraud count:\\n{fraud_receivers.to_string()}\\n\")\n",
    "else:\n",
    "    print('Could not identify sender/receiver columns for participation snapshot.')\n",
    "\n",
    "print('\\n' + '=' * 72)\n",
    "print('NUMERIC FEATURE CORRELATIONS WITH FRAUD (top 15 abs(r))')\n",
    "print('=' * 72)\n",
    "num_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c != label_col]\n",
    "if num_cols:\n",
    "    corrs = []\n",
    "    y = df[label_col].values\n",
    "    for c in num_cols:\n",
    "        x = pd.to_numeric(df[c], errors='coerce').fillna(0).values\n",
    "        try:\n",
    "            r, p = pointbiserialr(y, x)\n",
    "        except Exception:\n",
    "            r, p = np.nan, np.nan\n",
    "        corrs.append({'feature': c, 'r': r, 'p_value': p})\n",
    "    corr_df = pd.DataFrame(corrs)\n",
    "    corr_df['abs_r'] = corr_df['r'].abs()\n",
    "    corr_df = corr_df.sort_values(by='abs_r', ascending=False).head(15)\n",
    "    print(corr_df[['feature', 'r', 'p_value']].round(5).to_string(index=False))\n",
    "else:\n",
    "    print('No numeric columns (besides label) available for correlation analysis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# convert hex account numbers to int\n",
    "hex_to_int = np.vectorize(lambda x: int(x, 16))\n",
    "\n",
    "# create adjacency lists to represent the graph\n",
    "source = hex_to_int(working_trans['Account'])\n",
    "target = hex_to_int(working_trans['Account.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_nodes: 515080 num_edges: 5078345\n",
      "Data(edge_index=[2, 5078345], num_nodes=515080)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "\n",
    "# Map account IDs to a compact 0..N-1 index space to avoid huge sparse IDs\n",
    "# Concatenate unique accounts from source/target and factorize\n",
    "all_accounts = np.concatenate([source, target])\n",
    "unique_accounts, inverse_idx = np.unique(all_accounts, return_inverse=True)\n",
    "num_nodes = unique_accounts.shape[0]\n",
    "# Rebuild source/target as compact indices\n",
    "source_idx = inverse_idx[:source.shape[0]]\n",
    "target_idx = inverse_idx[source.shape[0]:]\n",
    "\n",
    "# Build edge_index\n",
    "edge_index = torch.tensor(np.vstack([source_idx, target_idx]), dtype=torch.long)\n",
    "\n",
    "# Create Data object\n",
    "data = Data(edge_index=edge_index, num_nodes=num_nodes)\n",
    "print('num_nodes:', num_nodes, 'num_edges:', edge_index.size(1))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 5078345], num_nodes=515080, edge_attr=[5078345, 3], edge_label=[5078345])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "\n",
    "# extract individual edge features\n",
    "time = pd.to_datetime(working_trans['Timestamp']).astype('int64') / 1e9\n",
    "amount_paid = working_trans['Amount Paid'].to_numpy()\n",
    "amount_received = working_trans['Amount Received'].to_numpy()\n",
    "\n",
    "# combine edge features into single tensor (standardised numeric block)\n",
    "numeric_features = np.column_stack([time, amount_paid, amount_received])\n",
    "scaler = StandardScaler()\n",
    "numeric_scaled = scaler.fit_transform(numeric_features)\n",
    "edge_features = torch.from_numpy(numeric_scaled).float()\n",
    "\n",
    "# create edge labels\n",
    "fraud_label = torch.tensor(working_trans['Is Laundering'].to_numpy(), dtype=torch.long)\n",
    "\n",
    "# attach features and labels to PyG Data\n",
    "data.edge_attr = edge_features\n",
    "data.edge_label = fraud_label\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "\n",
    "# Increase edge_batch_size if you have ample memory and want fewer edge chunks per epoch.\n",
    "edge_batch_size = 1024\n",
    "# Toggle GPU usage; set to False to keep everything on CPU even if CUDA is visible.\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# Ratio of sampled negatives to each positive edge during fallback training.\n",
    "neg_pos_ratio = 6.0\n",
    "# Scale factor applied to the empirical class imbalance when computing pos_weight.\n",
    "pos_weight_scale = 0.05\n",
    "# Optional manual override for pos_weight (set to a float to force a value).\n",
    "pos_weight_override = 1.0\n",
    "# Number of epochs to train for.\n",
    "epochs = 20\n",
    "# Hidden dimension for the GNN and edge classifier.\n",
    "num_hid = 64\n",
    "# Smaller learning rate to keep updates stable on imbalanced data.\n",
    "learn_rate = 3e-4\n",
    "# Weight decay for the optimizer.\n",
    "decay = 1e-4\n",
    "# Gradient clipping threshold (set <=0 to disable).\n",
    "grad_clip = 1.0\n",
    "# False positive rate target used when calibrating the decision threshold on validation data.\n",
    "fpr_target = 0.02\n",
    "# Minimum epochs before enabling regular recalibration so the loss can settle.\n",
    "calibrate_warmup = 6\n",
    "# How often (in epochs) to re-fit the validation ROC and refresh the threshold after warmup.\n",
    "calibrate_every = 2\n",
    "# Blend factor applied when updating the threshold (0=no change, 1=replace).\n",
    "threshold_blend = 0.5\n",
    "# Hard floor on the decision threshold to avoid runaway false positives.\n",
    "threshold_floor = 0.55\n",
    "# Hard ceiling on the decision threshold for numerical safety.\n",
    "threshold_ceiling = 0.995\n",
    "# Maximum allowed validation positive fraction before skipping a threshold update.\n",
    "max_val_pos_frac = 0.25\n",
    "# Maximum allowed validation FPR before skipping a threshold update.\n",
    "max_val_fpr = 0.15\n",
    "# Maximum allowed FPR on a raw-distribution sample when updating thresholds.\n",
    "max_full_sample_fpr = 0.06\n",
    "# Number of raw transactions sampled for full-distribution FPR checks.\n",
    "full_val_sample_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw-sample guard set: 50000 edges, fraud ratio 0.0012\n"
     ]
    }
   ],
   "source": [
    "# Build a raw-distribution sample for calibration guardrails\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data as PyGData\n",
    "\n",
    "# Use the same RNG seed to keep sampling deterministic across runs\n",
    "raw_base_df = working_trans\n",
    "raw_sample_size = min(int(full_val_sample_size), len(raw_base_df))\n",
    "if raw_sample_size <= 0:\n",
    "    raise ValueError('Dataset is empty; cannot create raw-sample guard for calibration.')\n",
    "raw_sample_idx = np.random.default_rng(RANDOM_SEED).choice(len(raw_base_df), size=raw_sample_size, replace=False)\n",
    "raw_sample_df = raw_base_df.iloc[raw_sample_idx].reset_index(drop=True)\n",
    "\n",
    "raw_source = hex_to_int(raw_sample_df['Account'])\n",
    "raw_target = hex_to_int(raw_sample_df['Account.1'])\n",
    "\n",
    "raw_all_accounts = np.concatenate([raw_source, raw_target])\n",
    "raw_unique_accounts, raw_inverse_idx = np.unique(raw_all_accounts, return_inverse=True)\n",
    "raw_num_nodes = raw_unique_accounts.shape[0]\n",
    "raw_source_idx = raw_inverse_idx[:raw_source.shape[0]]\n",
    "raw_target_idx = raw_inverse_idx[raw_source.shape[0]:]\n",
    "\n",
    "raw_edge_index = torch.tensor(np.vstack([raw_source_idx, raw_target_idx]), dtype=torch.long)\n",
    "raw_data = PyGData(edge_index=raw_edge_index, num_nodes=raw_num_nodes)\n",
    "\n",
    "raw_deg = torch.zeros((raw_num_nodes, 1), dtype=torch.float)\n",
    "raw_deg.scatter_add_(0, raw_edge_index[0].view(-1, 1), torch.ones((raw_edge_index.size(1), 1)))\n",
    "raw_data.x = raw_deg\n",
    "\n",
    "raw_time = pd.to_datetime(raw_sample_df['Timestamp']).astype('int64') / 1e9\n",
    "raw_amount_paid = raw_sample_df['Amount Paid'].to_numpy()\n",
    "raw_amount_received = raw_sample_df['Amount Received'].to_numpy()\n",
    "raw_numeric = np.column_stack([raw_time, raw_amount_paid, raw_amount_received])\n",
    "raw_numeric_scaled = scaler.transform(raw_numeric)\n",
    "raw_edge_attr = torch.from_numpy(raw_numeric_scaled).float()\n",
    "raw_edge_label = torch.tensor(raw_sample_df[LABEL_COL].to_numpy(), dtype=torch.long)\n",
    "\n",
    "raw_data.edge_attr = raw_edge_attr\n",
    "raw_data.edge_label = raw_edge_label\n",
    "\n",
    "print(f'Raw-sample guard set: {raw_sample_size} edges, fraud ratio {raw_edge_label.float().mean().item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masks set: 3047007 1015669 1015669\n",
      "Train positive ratio: 0.001019360963255167\n",
      "Val positive ratio: 0.0010190327884629369\n",
      "Test positive ratio: 0.0010200173128396273\n"
     ]
    }
   ],
   "source": [
    "# Stratified 60/20/20 split to keep the raw class imbalance in each subset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "num_edges = data.edge_index.size(1)\n",
    "all_indices = np.arange(num_edges)\n",
    "labels_np = data.edge_label.cpu().numpy()\n",
    "\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    all_indices,\n",
    "    test_size=0.4,\n",
    "    stratify=labels_np,\n",
    "    random_state=RANDOM_SEED,\n",
    "    shuffle=True,\n",
    "    )\n",
    "\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx,\n",
    "    test_size=0.5,\n",
    "    stratify=labels_np[temp_idx],\n",
    "    random_state=RANDOM_SEED,\n",
    "    shuffle=True,\n",
    "    )\n",
    "\n",
    "train_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
    "\n",
    "train_mask[torch.from_numpy(train_idx)] = True\n",
    "val_mask[torch.from_numpy(val_idx)] = True\n",
    "test_mask[torch.from_numpy(test_idx)] = True\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "print('Masks set:', train_mask.sum().item(), val_mask.sum().item(), test_mask.sum().item())\n",
    "print('Train positive ratio:', data.edge_label[train_mask].float().mean().item())\n",
    "print('Val positive ratio:', data.edge_label[val_mask].float().mean().item())\n",
    "print('Test positive ratio:', data.edge_label[test_mask].float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PyG GNN model and edge classification training (batched)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TransformerConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure data object exists with edge_index, edge_attr, edge_label, and masks\n",
    "assert data is not None, 'PyG Data not constructed yet'\n",
    "num_nodes = data.num_nodes\n",
    "num_edges = data.edge_index.size(1)\n",
    "if getattr(data, 'edge_attr', None) is None:\n",
    "    raise RuntimeError('Edge features missing. Run Cell 12 (edge feature construction) before this cell.')\n",
    "edge_feat_dim = data.edge_attr.size(1)\n",
    "\n",
    "# Create simple node features if none exist (e.g., degree or identity)\n",
    "if getattr(data, 'x', None) is None:\n",
    "    deg = torch.zeros((num_nodes, 1), dtype=torch.float)\n",
    "    deg.scatter_add_(0, data.edge_index[0].view(-1,1), torch.ones((num_edges,1)))\n",
    "    data.x = deg  # use degree as a simple node feature\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, edge_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = TransformerConv(in_channels, hidden_channels, edge_dim=edge_dim)\n",
    "        self.conv2 = TransformerConv(hidden_channels, hidden_channels, edge_dim=edge_dim)\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.conv1(x, edge_index, edge_attr=edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr=edge_attr)\n",
    "        return x\n",
    "\n",
    "class EdgeClassifier(nn.Module):\n",
    "    def __init__(self, node_hidden, edge_feat_dim, hidden=num_hid):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(node_hidden*2 + edge_feat_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        u, v = edge_index\n",
    "        h = torch.cat([x[u], x[v], edge_attr], dim=1)\n",
    "        return self.mlp(h).squeeze(-1)\n",
    "    def score_pairs(self, x, u, v, edge_attr):\n",
    "        h = torch.cat([x[u], x[v], edge_attr], dim=1)\n",
    "        return self.mlp(h).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU device: NVIDIA GeForce RTX 4070 Laptop GPU (index 0).\n",
      "Train edges: 3047007 | positives: 3106 (0.001019) | pos_weight 1.00 | neg/pos ratio target 6.0:1\n",
      "Backend OK: True | batch_size: 1024 | num_neighbors: [10, 5]\n"
     ]
    }
   ],
   "source": [
    "import importlib, math, torch, sys, subprocess, os\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "# Detect if neighbor sampling backend (pyg_lib or torch_sparse) is available\n",
    "backend_ok = bool(importlib.util.find_spec(\"pyg_lib\") or importlib.util.find_spec(\"torch_sparse\"))\n",
    "fallback_splits = {}\n",
    "if not backend_ok:\n",
    "    print(\"Neighbor sampling backend missing: install 'pyg-lib' (preferred) or 'torch-sparse'.\")\n",
    "    torch_ver = torch.__version__.split('+')[0]\n",
    "    cuda_ver = torch.version.cuda\n",
    "    if cuda_ver is None:\n",
    "        cuda_tag = 'cpu'\n",
    "    else:\n",
    "        cuda_tag = 'cu' + cuda_ver.replace('.', '')\n",
    "    index_url = f'https://data.pyg.org/whl/torch-{torch_ver}+{cuda_tag}.html'\n",
    "    print('Suggested install command (run in a terminal):')\n",
    "    print(f\"{sys.executable} -m pip install pyg-lib torch-sparse -f {index_url}\")\n",
    "    print('Fallback to full-graph edge training will be used until a backend is installed.')\n",
    "    print('Note: pyg_lib currently does not support some newer PyTorch versions; fallback will be used if install fails.')\n",
    "\n",
    "requested_gpu = use_gpu and torch.cuda.is_available()\n",
    "device = torch.device('cuda') if requested_gpu else torch.device('cpu')\n",
    "if device.type == 'cuda':\n",
    "    dev_index = device.index if device.index is not None else torch.cuda.current_device()\n",
    "    print(f'Using GPU device: {torch.cuda.get_device_name(dev_index)} (index {dev_index}).')\n",
    "else:\n",
    "    if use_gpu and not torch.cuda.is_available():\n",
    "        print('CUDA requested but not available; falling back to CPU.')\n",
    "    else:\n",
    "        print('Using CPU for training (set use_gpu=True and ensure CUDA availability to use GPU).')\n",
    "\n",
    "gnn = GNN(in_channels=data.x.size(1), hidden_channels=num_hid, edge_dim=edge_feat_dim).to(device)\n",
    "clf = EdgeClassifier(node_hidden=num_hid, edge_feat_dim=edge_feat_dim, hidden=num_hid*2).to(device)\n",
    "\n",
    "params = list(gnn.parameters()) + list(clf.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=learn_rate, weight_decay=decay)\n",
    "\n",
    "# Build edge_label_index, edge_label, and edge_label_attr tensors for each split\n",
    "train_edge_label_index = data.edge_index[:, data.train_mask]\n",
    "train_edge_label = data.edge_label[data.train_mask]\n",
    "train_edge_attr = data.edge_attr[data.train_mask]\n",
    "\n",
    "val_edge_label_index = data.edge_index[:, data.val_mask]\n",
    "val_edge_label = data.edge_label[data.val_mask]\n",
    "val_edge_attr = data.edge_attr[data.val_mask]\n",
    "\n",
    "test_edge_label_index = data.edge_index[:, data.test_mask]\n",
    "test_edge_label = data.edge_label[data.test_mask]\n",
    "test_edge_attr = data.edge_attr[data.test_mask]\n",
    "\n",
    "split_edge_label_attrs = {\n",
    "    'train': train_edge_attr,\n",
    "    'val': val_edge_attr,\n",
    "    'test': test_edge_attr,\n",
    "}\n",
    "\n",
    "def _make_edge_label_attr_transform(edge_attr_tensor):\n",
    "    def _transform(batch):\n",
    "        input_id = getattr(batch, 'input_id', None)\n",
    "        if input_id is None:\n",
    "            batch.edge_label_attr = None\n",
    "            return batch\n",
    "        idx = input_id.to(torch.long)\n",
    "        if idx.device.type != 'cpu':\n",
    "            idx = idx.cpu()\n",
    "        batch.edge_label_attr = edge_attr_tensor[idx]\n",
    "        return batch\n",
    "    return _transform\n",
    "\n",
    "train_pos = int(train_edge_label.sum().item())\n",
    "train_total = int(train_edge_label.numel())\n",
    "train_neg = max(train_total - train_pos, 0)\n",
    "base_pos_weight = (train_neg / max(train_pos, 1)) if train_pos > 0 else 1.0\n",
    "if pos_weight_override is not None:\n",
    "    pos_weight_value = float(pos_weight_override)\n",
    "else:\n",
    "    pos_weight_value = max(base_pos_weight * pos_weight_scale, 1.0)\n",
    "pos_weight_tensor = torch.tensor(pos_weight_value, dtype=torch.float, device=device)\n",
    "print(f'Train edges: {train_total} | positives: {train_pos} ({train_pos / max(train_total,1):.6f}) | pos_weight {pos_weight_value:.2f} | neg/pos ratio target {neg_pos_ratio:.1f}:1')\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "\n",
    "# Safer defaults for batch size & neighbors to reduce per-batch time/memory\n",
    "batch_size = edge_batch_size  # configurable via hyperparameter cell\n",
    "num_neighbors = [10, 5]  # fewer neighbors for smaller subgraphs\n",
    "\n",
    "fallback_mode = not backend_ok\n",
    "if backend_ok:\n",
    "    print(f'Backend OK: {backend_ok} | batch_size: {batch_size} | num_neighbors: {num_neighbors}')\n",
    "    train_loader = LinkNeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=num_neighbors,\n",
    "        batch_size=batch_size,\n",
    "        edge_label_index=train_edge_label_index,\n",
    "        edge_label=train_edge_label,\n",
    "        shuffle=True,\n",
    "        neg_sampling_ratio=0.0,\n",
    "        transform_sampler_output=_make_edge_label_attr_transform(train_edge_attr)\n",
    "    )\n",
    "    val_loader = LinkNeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=num_neighbors,\n",
    "        batch_size=batch_size,\n",
    "        edge_label_index=val_edge_label_index,\n",
    "        edge_label=val_edge_label,\n",
    "        shuffle=False,\n",
    "        neg_sampling_ratio=0.0,\n",
    "        transform_sampler_output=_make_edge_label_attr_transform(val_edge_attr)\n",
    "    )\n",
    "    test_loader = LinkNeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=num_neighbors,\n",
    "        batch_size=batch_size,\n",
    "        edge_label_index=test_edge_label_index,\n",
    "        edge_label=test_edge_label,\n",
    "        shuffle=False,\n",
    "        neg_sampling_ratio=0.0,\n",
    "        transform_sampler_output=_make_edge_label_attr_transform(test_edge_attr)\n",
    "    )\n",
    "else:\n",
    "    def _split_edges(mask):\n",
    "        return {\n",
    "            'edge_label_index': data.edge_index[:, mask],\n",
    "            'edge_label': data.edge_label[mask],\n",
    "            'edge_attr': data.edge_attr[mask]\n",
    "        }\n",
    "    fallback_splits = {\n",
    "        'train': _split_edges(data.train_mask),\n",
    "        'val': _split_edges(data.val_mask),\n",
    "        'test': _split_edges(data.test_mask)\n",
    "    }\n",
    "    train_loader = fallback_splits['train']\n",
    "    val_loader = fallback_splits['val']\n",
    "    test_loader = fallback_splits['test']\n",
    "    train_count = fallback_splits['train']['edge_label'].numel()\n",
    "    val_count = fallback_splits['val']['edge_label'].numel()\n",
    "    test_count = fallback_splits['test']['edge_label'].numel()\n",
    "    chunk_size = max(int(edge_batch_size), 1)\n",
    "    print(f'Fallback mode active on {device.type.upper()} device: full-graph embeddings with edge chunks of {chunk_size} (train edges {train_count}, val {val_count}, test {test_count}).')\n",
    "    print('edge_batch_size controls chunking in this mode; install pyg-lib or torch-sparse to enable true neighbor sampling.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_batches:   0%|          | 0/2976 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_batches:   0%|          | 0/2976 [00:00<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(500, 499)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 290\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs+\u001b[32m1\u001b[39m):\n\u001b[32m    289\u001b[39m     epoch_t0 = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     avg_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneg_pos_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m     epoch_time = time.time() - epoch_t0\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m math.isfinite(avg_loss):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 205\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(use_amp, log_every, neg_ratio)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[32m    204\u001b[39m     x = gnn(batch.x, batch.edge_index, batch.edge_attr)\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     label_edge_attr = \u001b[43m_gather_label_edge_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     logits = clf.score_pairs(x, batch.edge_label_index[\u001b[32m0\u001b[39m], batch.edge_label_index[\u001b[32m1\u001b[39m], label_edge_attr)\n\u001b[32m    207\u001b[39m     labels_float = batch.edge_label.float()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36m_gather_label_edge_attr\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m     19\u001b[39m lu = batch.edge_label_index[\u001b[32m0\u001b[39m].tolist()\n\u001b[32m     20\u001b[39m lv = batch.edge_label_index[\u001b[32m1\u001b[39m].tolist()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m idx = [\u001b[43mpos_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m u, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lu, lv)]\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m batch.edge_attr[idx].to(batch.edge_label_index.device)\n",
      "\u001b[31mKeyError\u001b[39m: (500, 499)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_curve\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import gc\n",
    "from contextlib import nullcontext\n",
    "\n",
    "\n",
    "def _gather_label_edge_attr(batch, source_attr=None):\n",
    "    if hasattr(batch, 'edge_label_attr') and batch.edge_label_attr is not None:\n",
    "        target_device = batch.edge_label_index.device\n",
    "        return batch.edge_label_attr.to(target_device)\n",
    "    if source_attr is not None and hasattr(batch, 'input_id') and batch.input_id is not None:\n",
    "        idx = batch.input_id.to(torch.long)\n",
    "        if idx.device.type != 'cpu':\n",
    "            idx_cpu = idx.cpu()\n",
    "        else:\n",
    "            idx_cpu = idx\n",
    "        gathered = source_attr[idx_cpu]\n",
    "        return gathered.to(batch.edge_label_index.device)\n",
    "    # Fallback path: reconstruct positions when LinkNeighborLoader did not supply edge_label_attr\n",
    "    e_u = batch.edge_index[0].tolist()\n",
    "    e_v = batch.edge_index[1].tolist()\n",
    "    pos_map = {(eu, ev): i for i, (eu, ev) in enumerate(zip(e_u, e_v))}\n",
    "    lu = batch.edge_label_index[0].tolist()\n",
    "    lv = batch.edge_label_index[1].tolist()\n",
    "    idx = [pos_map[(u, v)] for u, v in zip(lu, lv)]\n",
    "    return batch.edge_attr[idx].to(batch.edge_label_index.device)\n",
    "\n",
    "\n",
    "def _iter_fallback_chunks(split, chunk_size, index_subset=None):\n",
    "    edge_index = split['edge_label_index']\n",
    "    edge_attr = split['edge_attr']\n",
    "    edge_label = split['edge_label']\n",
    "    if index_subset is None:\n",
    "        indices = torch.arange(edge_label.size(0))\n",
    "    else:\n",
    "        indices = index_subset\n",
    "    total = indices.numel()\n",
    "    for start in range(0, total, chunk_size):\n",
    "        sel = indices[start:min(start + chunk_size, total)]\n",
    "        yield edge_index[:, sel], edge_attr[sel], edge_label[sel]\n",
    "\n",
    "\n",
    "def _sample_balanced_indices(labels, ratio):\n",
    "    pos_idx = torch.nonzero(labels == 1, as_tuple=False).view(-1)\n",
    "    neg_idx = torch.nonzero(labels == 0, as_tuple=False).view(-1)\n",
    "    if pos_idx.numel() == 0:\n",
    "        return torch.zeros(0, dtype=torch.long)\n",
    "    neg_needed = int(math.ceil(pos_idx.numel() * ratio))\n",
    "    if neg_idx.numel() == 0:\n",
    "        combined = pos_idx\n",
    "    else:\n",
    "        neg_needed = min(max(neg_needed, pos_idx.numel()), neg_idx.numel())\n",
    "        perm = torch.randperm(neg_idx.numel())\n",
    "        sampled_neg = neg_idx[perm[:neg_needed]]\n",
    "        combined = torch.cat([pos_idx, sampled_neg])\n",
    "    shuffle = torch.randperm(combined.numel())\n",
    "    return combined[shuffle]\n",
    "\n",
    "\n",
    "def _select_threshold(y_true, probs, target_fpr=0.02):\n",
    "    if y_true.size == 0:\n",
    "        return 0.5\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, probs)\n",
    "    if np.isnan(thresholds).all():\n",
    "        return 0.5\n",
    "    # Remove infinities for stability\n",
    "    finite_mask = np.isfinite(thresholds)\n",
    "    fpr, tpr, thresholds = fpr[finite_mask], tpr[finite_mask], thresholds[finite_mask]\n",
    "    if thresholds.size == 0:\n",
    "        return 0.5\n",
    "    if target_fpr is not None:\n",
    "        ok = np.where(fpr <= target_fpr)[0]\n",
    "        if ok.size > 0:\n",
    "            idx = ok[np.argmax(tpr[ok])]\n",
    "        else:\n",
    "            idx = np.argmin(fpr)\n",
    "    else:\n",
    "        youden = tpr - fpr\n",
    "        idx = np.argmax(youden)\n",
    "    thr = thresholds[idx]\n",
    "    if np.isnan(thr):\n",
    "        thr = 0.5\n",
    "    return float(np.clip(thr, 1e-6, 1 - 1e-6))\n",
    "\n",
    "\n",
    "def _summarise_predictions(y_true, probs, threshold):\n",
    "    preds = (probs >= threshold).astype(np.int64)\n",
    "    if preds.size == 0:\n",
    "        return {\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1': 0.0,\n",
    "            'fpr': 0.0,\n",
    "            'acc': 0.0,\n",
    "            'pos_frac': 0.0,\n",
    "            'threshold': threshold,\n",
    "            'preds': preds,\n",
    "            'probs': probs,\n",
    "            'labels': y_true\n",
    "        }\n",
    "    acc = (preds == y_true).mean()\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(y_true, preds, average='binary', zero_division=0)\n",
    "    cm = confusion_matrix(y_true, preds, labels=[0, 1])\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        fpr = fp / max(tn + fp, 1)\n",
    "    else:\n",
    "        fpr = 0.0\n",
    "    pos_frac = preds.mean()\n",
    "    return {\n",
    "        'precision': float(pr),\n",
    "        'recall': float(rc),\n",
    "        'f1': float(f1),\n",
    "        'fpr': float(fpr),\n",
    "        'acc': float(acc),\n",
    "        'pos_frac': float(pos_frac),\n",
    "        'threshold': float(threshold),\n",
    "        'preds': preds,\n",
    "        'probs': probs,\n",
    "        'labels': y_true\n",
    "    }\n",
    "\n",
    "\n",
    "def _refine_threshold_with_fpr_limit(probs, labels, max_fpr, fallback_threshold):\n",
    "    if probs.size == 0:\n",
    "        return float('nan')\n",
    "    fpr, tpr, thresholds = roc_curve(labels, probs)\n",
    "    finite_mask = np.isfinite(thresholds)\n",
    "    fpr, thresholds = fpr[finite_mask], thresholds[finite_mask]\n",
    "    if thresholds.size == 0:\n",
    "        return float('nan')\n",
    "    valid_idx = np.where(fpr <= max_fpr)[0]\n",
    "    if valid_idx.size == 0:\n",
    "        tightened = max(fallback_threshold, np.max(thresholds))\n",
    "        tightened = float(np.clip(tightened, 1e-6, 0.999999))\n",
    "        return tightened\n",
    "    candidate = thresholds[valid_idx].max()\n",
    "    if not np.isfinite(candidate):\n",
    "        return float('nan')\n",
    "    return float(np.clip(candidate, 1e-6, 1 - 1e-6))\n",
    "\n",
    "\n",
    "def _evaluate_raw_sample(threshold):\n",
    "    if 'raw_data' not in globals():\n",
    "        return None\n",
    "    gnn.eval(); clf.eval()\n",
    "    with torch.no_grad():\n",
    "        raw_batch = raw_data.clone().to(device)\n",
    "        raw_logits = clf.score_pairs(\n",
    "            gnn(raw_batch.x, raw_batch.edge_index, raw_batch.edge_attr),\n",
    "            raw_batch.edge_index[0],\n",
    "            raw_batch.edge_index[1],\n",
    "            raw_batch.edge_attr\n",
    "        )\n",
    "        probs = torch.sigmoid(raw_logits).cpu().numpy().astype(np.float32)\n",
    "        labels = raw_batch.edge_label.cpu().numpy().astype(np.int64)\n",
    "    return _summarise_predictions(labels, probs, threshold)\n",
    "\n",
    "\n",
    "def train_one_epoch(use_amp=True, log_every=200, neg_ratio=3.0):\n",
    "    gnn.train(); clf.train()\n",
    "    amp_enabled = use_amp and (device.type == 'cuda') and not fallback_mode\n",
    "    scaler = torch.amp.GradScaler('cuda') if amp_enabled else None\n",
    "    clip_enabled = (grad_clip is not None) and (grad_clip > 0)\n",
    "    if fallback_mode:\n",
    "        optimizer.zero_grad()\n",
    "        x = gnn(data.x.to(device), data.edge_index.to(device), data.edge_attr.to(device))\n",
    "        split = fallback_splits['train']\n",
    "        labels_cpu = split['edge_label'].cpu()\n",
    "        selected_indices = _sample_balanced_indices(labels_cpu, neg_ratio)\n",
    "        if selected_indices.numel() == 0:\n",
    "            print('No positive edges found in training split; cannot update model.')\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            return float('nan')\n",
    "        total_edges = selected_indices.numel()\n",
    "        chunk_size = max(int(edge_batch_size), 1)\n",
    "        loss_terms = []\n",
    "        for edge_idx_chunk, edge_attr_chunk, label_chunk in _iter_fallback_chunks(split, chunk_size, selected_indices):\n",
    "            u = edge_idx_chunk[0].to(device)\n",
    "            v = edge_idx_chunk[1].to(device)\n",
    "            edge_attr = torch.nan_to_num(edge_attr_chunk, nan=0.0, posinf=0.0, neginf=0.0).to(device)\n",
    "            edge_label = label_chunk.to(device).float()\n",
    "            logits = clf.score_pairs(x, u, v, edge_attr)\n",
    "            loss = criterion(logits, edge_label)\n",
    "            if not torch.isfinite(loss):\n",
    "                print('Non-finite loss encountered in fallback chunk; try lowering pos_weight or learning rate.')\n",
    "                return float('nan')\n",
    "            loss_terms.append(loss * edge_label.numel())\n",
    "        if not loss_terms:\n",
    "            print('Balanced sampling produced no batches; skipping epoch.')\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            return 0.0\n",
    "        loss_total = torch.stack(loss_terms).sum() / max(total_edges, 1)\n",
    "        loss_total.backward()\n",
    "        if clip_enabled:\n",
    "            torch.nn.utils.clip_grad_norm_(params, max_norm=grad_clip)\n",
    "        optimizer.step()\n",
    "        return float(loss_total.detach().cpu())\n",
    "    # Neighbor-sampling path (backend available)\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "    t0 = time.time()\n",
    "    for i, batch in enumerate(tqdm(train_loader, desc='train_batches'), 1):\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        context = torch.amp.autocast('cuda') if scaler is not None else nullcontext()\n",
    "        with context:\n",
    "            x = gnn(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            label_edge_attr = _gather_label_edge_attr(batch, split_edge_label_attrs['train'])\n",
    "            logits = clf.score_pairs(x, batch.edge_label_index[0], batch.edge_label_index[1], label_edge_attr)\n",
    "            labels_float = batch.edge_label.float()\n",
    "            loss = criterion(logits, labels_float)\n",
    "        if not torch.isfinite(loss):\n",
    "            # print(f'  batch {i} produced non-finite loss; skipping update.')\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            continue\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            if clip_enabled:\n",
    "                torch.nn.utils.clip_grad_norm_(params, max_norm=grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            if clip_enabled:\n",
    "                torch.nn.utils.clip_grad_norm_(params, max_norm=grad_clip)\n",
    "            optimizer.step()\n",
    "        batch_size_local = batch.edge_label.numel()\n",
    "        total_loss += loss.item() * batch_size_local\n",
    "        total_count += batch_size_local\n",
    "        if i % max(log_every, 1) == 0:\n",
    "            print(f'  batch {i} | batch_loss {loss.item():.4f} | elapsed {time.time()-t0:.1f}s')\n",
    "    return total_loss / max(total_count, 1)\n",
    "\n",
    "\n",
    "def evaluate_split(split_name, threshold=None, calibrate=False):\n",
    "    gnn.eval(); clf.eval()\n",
    "    chunk_size = max(int(edge_batch_size), 1)\n",
    "    labels_list = []\n",
    "    probs_list = []\n",
    "    if fallback_mode:\n",
    "        split = fallback_splits[split_name]\n",
    "        with torch.no_grad():\n",
    "            x = gnn(data.x.to(device), data.edge_index.to(device), data.edge_attr.to(device))\n",
    "            for edge_idx_chunk, edge_attr_chunk, label_chunk in _iter_fallback_chunks(split, chunk_size):\n",
    "                u = edge_idx_chunk[0].to(device)\n",
    "                v = edge_idx_chunk[1].to(device)\n",
    "                edge_attr = torch.nan_to_num(edge_attr_chunk, nan=0.0, posinf=0.0, neginf=0.0).to(device)\n",
    "                logits = clf.score_pairs(x, u, v, edge_attr)\n",
    "                probs = torch.sigmoid(logits).detach().cpu()\n",
    "                probs_list.append(probs)\n",
    "                labels_list.append(label_chunk.detach().cpu())\n",
    "    else:\n",
    "        loader_map = {\n",
    "            'train': (train_loader, split_edge_label_attrs['train']),\n",
    "            'val': (val_loader, split_edge_label_attrs['val']),\n",
    "            'test': (test_loader, split_edge_label_attrs['test']),\n",
    "        }\n",
    "        loader, source_attr = loader_map[split_name]\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader, desc=f'{split_name}_batches'):\n",
    "                batch = batch.to(device)\n",
    "                x = gnn(batch.x, batch.edge_index, batch.edge_attr)\n",
    "                label_edge_attr = _gather_label_edge_attr(batch, source_attr)\n",
    "                logits = clf.score_pairs(x, batch.edge_label_index[0], batch.edge_label_index[1], label_edge_attr)\n",
    "                probs = torch.sigmoid(logits).detach().cpu()\n",
    "                probs_list.append(probs)\n",
    "                labels_list.append(batch.edge_label.detach().cpu())\n",
    "    if not labels_list:\n",
    "        empty = np.array([])\n",
    "        return {\n",
    "            'acc': 0.0,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1': 0.0,\n",
    "            'fpr': 0.0,\n",
    "            'pos_frac': 0.0,\n",
    "            'threshold': threshold if threshold is not None else 0.5,\n",
    "            'preds': empty,\n",
    "            'probs': empty,\n",
    "            'labels': empty\n",
    "        }\n",
    "    labels = torch.cat(labels_list).numpy().astype(np.int64)\n",
    "    probs = torch.cat(probs_list).numpy().astype(np.float32)\n",
    "    if calibrate or threshold is None:\n",
    "        chosen_threshold = _select_threshold(labels, probs, target_fpr=fpr_target)\n",
    "    else:\n",
    "        chosen_threshold = threshold\n",
    "    summary = _summarise_predictions(labels, probs, chosen_threshold)\n",
    "    return summary\n",
    "\n",
    "\n",
    "log_every = 200 if not fallback_mode else 0\n",
    "calibrated_threshold = 0.5\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_t0 = time.time()\n",
    "    avg_loss = train_one_epoch(use_amp=(device.type == 'cuda'), log_every=log_every, neg_ratio=neg_pos_ratio)\n",
    "    epoch_time = time.time() - epoch_t0\n",
    "    if not math.isfinite(avg_loss):\n",
    "        print(f'Epoch {epoch:02d} skipped due to non-finite loss.')\n",
    "        continue\n",
    "    warmup_ready = epoch >= calibrate_warmup\n",
    "    should_calibrate = (epoch == 1) or (warmup_ready and ((epoch - calibrate_warmup) % max(calibrate_every, 1) == 0))\n",
    "    train_metrics = evaluate_split('train', threshold=calibrated_threshold)\n",
    "    val_metrics = evaluate_split('val', threshold=calibrated_threshold, calibrate=should_calibrate)\n",
    "    if should_calibrate:\n",
    "        new_threshold = val_metrics['threshold']\n",
    "        allow_update = np.isfinite(new_threshold)\n",
    "        if allow_update and val_metrics['pos_frac'] > max_val_pos_frac:\n",
    "            print(f\"  skip threshold update: val_pos_frac {val_metrics['pos_frac']:.3f} exceeds {max_val_pos_frac:.3f}\")\n",
    "            allow_update = False\n",
    "        if allow_update and val_metrics['fpr'] > max_val_fpr:\n",
    "            print(f\"  skip threshold update: val_fpr {val_metrics['fpr']:.3f} exceeds {max_val_fpr:.3f}\")\n",
    "            allow_update = False\n",
    "        raw_guard_metrics = None\n",
    "        if allow_update and 'raw_data' in globals():\n",
    "            baseline_guard = _evaluate_raw_sample(new_threshold)\n",
    "            if baseline_guard is not None and baseline_guard['probs'].size > 0:\n",
    "                print(\n",
    "                    f\"  raw-sample baseline -> precision {baseline_guard['precision']:.3f}, \",\n",
    "                    f\"recall {baseline_guard['recall']:.3f}, fpr {baseline_guard['fpr']:.4f} at threshold {new_threshold:.4f}\"\n",
    "                )\n",
    "                refined_threshold = _refine_threshold_with_fpr_limit(\n",
    "                    baseline_guard['probs'],\n",
    "                    baseline_guard['labels'],\n",
    "                    max_full_sample_fpr,\n",
    "                    new_threshold\n",
    "                )\n",
    "                if not np.isfinite(refined_threshold):\n",
    "                    print(\n",
    "                        f\"  skip threshold update: raw_sample_fpr {baseline_guard['fpr']:.3f} exceeds {max_full_sample_fpr:.3f} and no tighter threshold meets the limit\"\n",
    "                    )\n",
    "                    allow_update = False\n",
    "                else:\n",
    "                    refined_threshold = float(np.clip(refined_threshold, threshold_floor, threshold_ceiling))\n",
    "                    if abs(refined_threshold - new_threshold) < 1e-6:\n",
    "                        raw_guard_metrics = baseline_guard\n",
    "                    else:\n",
    "                        raw_guard_metrics = _summarise_predictions(\n",
    "                            baseline_guard['labels'],\n",
    "                            baseline_guard['probs'],\n",
    "                            refined_threshold\n",
    "                        )\n",
    "                    if raw_guard_metrics['fpr'] > max_full_sample_fpr:\n",
    "                        print(\n",
    "                            f\"  raw-sample FPR {raw_guard_metrics['fpr']:.3f} still above {max_full_sample_fpr:.3f}; applying best available threshold {refined_threshold:.4f} and flagging for retraining.\"\n",
    "                        )\n",
    "                    elif refined_threshold > new_threshold + 1e-6:\n",
    "                        print(\n",
    "                            f\"  raw-sample threshold raised from {new_threshold:.4f} to {refined_threshold:.4f} to respect FPR <= {max_full_sample_fpr:.3f}\"\n",
    "                        )\n",
    "                    new_threshold = refined_threshold\n",
    "            elif baseline_guard is None or baseline_guard['probs'].size == 0:\n",
    "                print('  raw-sample guard skipped: sample empty or unavailable')\n",
    "        if allow_update:\n",
    "            if epoch == 1:\n",
    "                calibrated_threshold = float(np.clip(new_threshold, threshold_floor, threshold_ceiling))\n",
    "            else:\n",
    "                blended = ((1.0 - threshold_blend) * calibrated_threshold) + (threshold_blend * new_threshold)\n",
    "                calibrated_threshold = float(np.clip(blended, threshold_floor, threshold_ceiling))\n",
    "            if raw_guard_metrics is not None:\n",
    "                print(\n",
    "                    f\"  raw-sample metrics -> precision {raw_guard_metrics['precision']:.3f}, \",\n",
    "                    f\"recall {raw_guard_metrics['recall']:.3f}, fpr {raw_guard_metrics['fpr']:.4f}\"\n",
    "                )\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | loss {avg_loss:.4f} | time {epoch_time:.1f}s | \",\n",
    "        f\"train_acc {train_metrics['acc']:.3f} | val_acc {val_metrics['acc']:.3f} | \",\n",
    "        f\"val_precision {val_metrics['precision']:.3f} | val_recall {val_metrics['recall']:.3f} | \",\n",
    "        f\"val_f1 {val_metrics['f1']:.3f} | val_fpr {val_metrics['fpr']:.4f} | val_thresh {calibrated_threshold:.4f} | \",\n",
    "        f\"val_pos_frac {val_metrics['pos_frac']:.5f}\"\n",
    "    )\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m13 packages\u001b[0m \u001b[2min 157ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/2] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 136ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib-inline\u001b[0m\u001b[2m==0.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtraitlets\u001b[0m\u001b[2m==5.14.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install matplotlib matplotlib-inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.840266\n",
      "Test precision: 0.001324\n",
      "Test recall: 0.206564\n",
      "Test F1: 0.002631\n",
      "False positive rate: 0.159087\n",
      "Predicted positive fraction: 0.159136\n",
      "Decision threshold: 0.873357\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m cm = confusion_matrix(labels, preds, labels=[\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m])\n\u001b[32m     16\u001b[39m disp = ConfusionMatrixDisplay(cm, display_labels=[\u001b[33m'\u001b[39m\u001b[33mNon-fraud\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFraud\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mdisp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues_format\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43md\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cm.shape == (\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m):\n\u001b[32m     19\u001b[39m     tn, fp, fn, tp = cm.ravel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/SFSU/CSC871/csc871-anti-money-laundering-ibm-gnn/.venv/lib/python3.12/site-packages/sklearn/metrics/_plot/confusion_matrix.py:139\u001b[39m, in \u001b[36mConfusionMatrixDisplay.plot\u001b[39m\u001b[34m(self, include_values, cmap, xticks_rotation, values_format, ax, colorbar, im_kw, text_kw)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot\u001b[39m(\n\u001b[32m     89\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     90\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     98\u001b[39m     text_kw=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     99\u001b[39m ):\n\u001b[32m    100\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Plot visualization.\u001b[39;00m\n\u001b[32m    101\u001b[39m \n\u001b[32m    102\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    137\u001b[39m \u001b[33;03m        that contains all the information to plot the confusion matrix.\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[43mcheck_matplotlib_support\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mConfusionMatrixDisplay.plot\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/SFSU/CSC871/csc871-anti-money-laundering-ibm-gnn/.venv/lib/python3.12/site-packages/sklearn/utils/_optional_dependencies.py:17\u001b[39m, in \u001b[36mcheck_matplotlib_support\u001b[39m\u001b[34m(caller_name)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Raise ImportError with detailed error message if mpl is not installed.\u001b[39;00m\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[33;03mPlot utilities like any of the Display's plotting functions should lazily import\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[33;03m    The name of the caller that requires matplotlib.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     20\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m requires matplotlib. You can install matplotlib with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`pip install matplotlib`\u001b[39m\u001b[33m\"\u001b[39m.format(caller_name)\n\u001b[32m     22\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/SFSU/CSC871/csc871-anti-money-laundering-ibm-gnn/.venv/lib/python3.12/site-packages/matplotlib/__init__.py:1299\u001b[39m\n\u001b[32m   1295\u001b[39m     rcParams[\u001b[33m'\u001b[39m\u001b[33mbackend_fallback\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.environ.get(\u001b[33m'\u001b[39m\u001b[33mMPLBACKEND\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1299\u001b[39m     \u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbackend\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m = os.environ.get(\u001b[33m'\u001b[39m\u001b[33mMPLBACKEND\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_backend\u001b[39m(*, auto_select=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1303\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1304\u001b[39m \u001b[33;03m    Return the name of the current backend.\u001b[39;00m\n\u001b[32m   1305\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1323\u001b[39m \u001b[33;03m    matplotlib.use\u001b[39;00m\n\u001b[32m   1324\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/SFSU/CSC871/csc871-anti-money-laundering-ibm-gnn/.venv/lib/python3.12/site-packages/matplotlib/__init__.py:774\u001b[39m, in \u001b[36mRcParams.__setitem__\u001b[39m\u001b[34m(self, key, val)\u001b[39m\n\u001b[32m    772\u001b[39m         cval = \u001b[38;5;28mself\u001b[39m.validate[key](val)\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mve\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    775\u001b[39m     \u001b[38;5;28mself\u001b[39m._set(key, cval)\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[31mValueError\u001b[39m: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "test_metrics = evaluate_split('test', threshold=calibrated_threshold)\n",
    "labels = test_metrics['labels']\n",
    "preds = test_metrics['preds']\n",
    "print('Test accuracy:', round(test_metrics['acc'], 6))\n",
    "print('Test precision:', round(test_metrics['precision'], 6))\n",
    "print('Test recall:', round(test_metrics['recall'], 6))\n",
    "print('Test F1:', round(test_metrics['f1'], 6))\n",
    "print('False positive rate:', round(test_metrics['fpr'], 6))\n",
    "print('Predicted positive fraction:', round(test_metrics['pos_frac'], 6))\n",
    "print('Decision threshold:', round(test_metrics['threshold'], 6))\n",
    "\n",
    "if labels.size > 0 and preds.size > 0:\n",
    "    cm = confusion_matrix(labels, preds, labels=[0, 1])\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=['Non-fraud', 'Fraud'])\n",
    "    disp.plot(values_format='d')\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        print(f'TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}')\n",
    "    else:\n",
    "        print(f'Confusion matrix shape unexpected: {cm.shape}')\n",
    "else:\n",
    "    print('Not enough classes in test to compute CM/recall/FPR.')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Full-dataset evaluation ===\n",
      "Accuracy: 0.840537\n",
      "Precision: 0.0013\n",
      "Recall: 0.202627\n",
      "F1: 0.002584\n",
      "False positive rate: 0.158812\n",
      "Predicted positive fraction: 0.158856\n",
      "Decision threshold: 0.873357\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m cm_full = confusion_matrix(labels_full, full_metrics[\u001b[33m'\u001b[39m\u001b[33mpreds\u001b[39m\u001b[33m'\u001b[39m], labels=[\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m])\n\u001b[32m     34\u001b[39m disp_full = ConfusionMatrixDisplay(cm_full, display_labels=[\u001b[33m'\u001b[39m\u001b[33mNon-fraud\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFraud\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mdisp_full\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues_format\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43md\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cm_full.shape == (\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m):\n\u001b[32m     37\u001b[39m     tn, fp, fn, tp = cm_full.ravel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/SFSU/CSC871/csc871-anti-money-laundering-ibm-gnn/.venv/lib/python3.12/site-packages/sklearn/metrics/_plot/confusion_matrix.py:139\u001b[39m, in \u001b[36mConfusionMatrixDisplay.plot\u001b[39m\u001b[34m(self, include_values, cmap, xticks_rotation, values_format, ax, colorbar, im_kw, text_kw)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot\u001b[39m(\n\u001b[32m     89\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     90\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     98\u001b[39m     text_kw=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     99\u001b[39m ):\n\u001b[32m    100\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Plot visualization.\u001b[39;00m\n\u001b[32m    101\u001b[39m \n\u001b[32m    102\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    137\u001b[39m \u001b[33;03m        that contains all the information to plot the confusion matrix.\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[43mcheck_matplotlib_support\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mConfusionMatrixDisplay.plot\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/SFSU/CSC871/csc871-anti-money-laundering-ibm-gnn/.venv/lib/python3.12/site-packages/sklearn/utils/_optional_dependencies.py:17\u001b[39m, in \u001b[36mcheck_matplotlib_support\u001b[39m\u001b[34m(caller_name)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Raise ImportError with detailed error message if mpl is not installed.\u001b[39;00m\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[33;03mPlot utilities like any of the Display's plotting functions should lazily import\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[33;03m    The name of the caller that requires matplotlib.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     20\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m requires matplotlib. You can install matplotlib with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`pip install matplotlib`\u001b[39m\u001b[33m\"\u001b[39m.format(caller_name)\n\u001b[32m     22\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/SFSU/CSC871/csc871-anti-money-laundering-ibm-gnn/.venv/lib/python3.12/site-packages/matplotlib/__init__.py:1299\u001b[39m\n\u001b[32m   1295\u001b[39m     rcParams[\u001b[33m'\u001b[39m\u001b[33mbackend_fallback\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.environ.get(\u001b[33m'\u001b[39m\u001b[33mMPLBACKEND\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1299\u001b[39m     \u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbackend\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m = os.environ.get(\u001b[33m'\u001b[39m\u001b[33mMPLBACKEND\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_backend\u001b[39m(*, auto_select=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1303\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1304\u001b[39m \u001b[33;03m    Return the name of the current backend.\u001b[39;00m\n\u001b[32m   1305\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1323\u001b[39m \u001b[33;03m    matplotlib.use\u001b[39;00m\n\u001b[32m   1324\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/SFSU/CSC871/csc871-anti-money-laundering-ibm-gnn/.venv/lib/python3.12/site-packages/matplotlib/__init__.py:774\u001b[39m, in \u001b[36mRcParams.__setitem__\u001b[39m\u001b[34m(self, key, val)\u001b[39m\n\u001b[32m    772\u001b[39m         cval = \u001b[38;5;28mself\u001b[39m.validate[key](val)\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mve\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    775\u001b[39m     \u001b[38;5;28mself\u001b[39m._set(key, cval)\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[31mValueError\u001b[39m: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']"
     ]
    }
   ],
   "source": [
    "# Evaluate on the full dataset using the trained model (reuse the in-memory graph)\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "assert 'data' in globals(), \"Construct the PyG graph before running this cell.\"\n",
    "assert 'gnn' in globals() and 'clf' in globals(), \"Train the model before running a full-dataset evaluation.\"\n",
    "\n",
    "full_batch = data.clone().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_full = gnn(full_batch.x, full_batch.edge_index, full_batch.edge_attr)\n",
    "    logits_full = clf.score_pairs(\n",
    "        x_full,\n",
    "        full_batch.edge_index[0],\n",
    "        full_batch.edge_index[1],\n",
    "        full_batch.edge_attr\n",
    "    )\n",
    "    probs_full = torch.sigmoid(logits_full).cpu().numpy().astype(np.float32)\n",
    "\n",
    "labels_full = full_batch.edge_label.cpu().numpy().astype(np.int64)\n",
    "full_metrics = _summarise_predictions(labels_full, probs_full, calibrated_threshold)\n",
    "\n",
    "print('\\n=== Full-dataset evaluation ===')\n",
    "print('Accuracy:', round(full_metrics['acc'], 6))\n",
    "print('Precision:', round(full_metrics['precision'], 6))\n",
    "print('Recall:', round(full_metrics['recall'], 6))\n",
    "print('F1:', round(full_metrics['f1'], 6))\n",
    "print('False positive rate:', round(full_metrics['fpr'], 6))\n",
    "print('Predicted positive fraction:', round(full_metrics['pos_frac'], 6))\n",
    "print('Decision threshold:', round(full_metrics['threshold'], 6))\n",
    "\n",
    "if labels_full.size > 0 and full_metrics['preds'].size > 0:\n",
    "    cm_full = confusion_matrix(labels_full, full_metrics['preds'], labels=[0, 1])\n",
    "    disp_full = ConfusionMatrixDisplay(cm_full, display_labels=['Non-fraud', 'Fraud'])\n",
    "    disp_full.plot(values_format='d')\n",
    "    if cm_full.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm_full.ravel()\n",
    "        print(f'TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}')\n",
    "    else:\n",
    "        print(f'Confusion matrix shape unexpected: {cm_full.shape}')\n",
    "else:\n",
    "    print('Not enough classes to compute confusion matrix metrics on full dataset.')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOv070Al0cK/8N1oI0v7dEu",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "csc865-anti-money-laundering-ibm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
