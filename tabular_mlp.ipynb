{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767ac323",
   "metadata": {},
   "source": [
    "# Tabular MLP Baseline\n",
    "Use this notebook to train a feed-forward network with categorical embeddings on the AML transactions dataset. Adjust the configuration in Cell 2 before running subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4c22e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from typing import Any, Dict, Iterable, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import auc, precision_recall_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "CONFIG = {\n",
    "    'dataset': 'dataset/HI-Small_Trans.csv',\n",
    "    'epochs': 20,\n",
    "    'batch_size': 8192,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'hidden_dims': '256,128,64',\n",
    "    'dropout': 0.2,\n",
    "    'test_size': 0.2,\n",
    "    'val_size': 0.1,\n",
    "    'random_state': 42,\n",
    "    'base_embed_dim': 8,\n",
    "    'max_embed_dim': 64,\n",
    "    'device': 'auto',\n",
    "    'target_fpr': 0.05,\n",
    "    'report': None,\n",
    "    'max_samples': None,\n",
    "    'num_workers': 0,\n",
    "}\n",
    "\n",
    "cfg = SimpleNamespace(**CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f37d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        numeric: np.ndarray,\n",
    "        categorical: np.ndarray,\n",
    "        labels: np.ndarray,\n",
    "    ) -> None:\n",
    "        self.numeric = torch.tensor(numeric, dtype=torch.float32)\n",
    "        self.categorical = torch.tensor(categorical, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        return self.numeric[idx], self.categorical[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "class TabularMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_numeric: int,\n",
    "        categorical_cardinalities: List[int],\n",
    "        hidden_dims: List[int],\n",
    "        dropout: float,\n",
    "        base_embed_dim: int,\n",
    "        max_embed_dim: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        embeddings: List[nn.Module] = []\n",
    "        embed_total = 0\n",
    "        for cardinality in categorical_cardinalities:\n",
    "            if cardinality <= 1:\n",
    "                embeddings.append(None)  # type: ignore[arg-type]\n",
    "                continue\n",
    "            dim = min(max_embed_dim, max(4, int(math.ceil(cardinality ** 0.25 * base_embed_dim))))\n",
    "            emb = nn.Embedding(cardinality, dim, padding_idx=0)\n",
    "            nn.init.xavier_uniform_(emb.weight)\n",
    "            embeddings.append(emb)\n",
    "            embed_total += dim\n",
    "        self.embeddings = nn.ModuleList([emb for emb in embeddings if emb is not None])\n",
    "        self.embedding_indices = [idx for idx, emb in enumerate(embeddings) if emb is not None]\n",
    "\n",
    "        input_dim = num_numeric + embed_total\n",
    "        layers: List[nn.Module] = []\n",
    "        prev_dim = input_dim\n",
    "        for width in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, width))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(width))\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = width\n",
    "        self.mlp = nn.Sequential(*layers) if layers else nn.Identity()\n",
    "        self.head = nn.Linear(prev_dim, 1)\n",
    "\n",
    "    def forward(self, numeric: torch.Tensor, categorical: torch.Tensor) -> torch.Tensor:\n",
    "        pieces: List[torch.Tensor] = []\n",
    "        if numeric.shape[1] > 0:\n",
    "            pieces.append(numeric)\n",
    "        if self.embeddings:\n",
    "            embeds: List[torch.Tensor] = []\n",
    "            for module, idx in zip(self.embeddings, self.embedding_indices):\n",
    "                embeds.append(module(categorical[:, idx]))\n",
    "            pieces.append(torch.cat(embeds, dim=1))\n",
    "        features = pieces[0] if len(pieces) == 1 else torch.cat(pieces, dim=1)\n",
    "        hidden = self.mlp(features) if isinstance(self.mlp, nn.Sequential) else self.mlp(features)\n",
    "        return self.head(hidden).squeeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb5a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transactions(path: Path, max_samples: int | None) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    if max_samples is not None and len(df) > max_samples:\n",
    "        df = df.sample(max_samples, random_state=42).reset_index(drop=True)\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            'From Bank': 'from_bank',\n",
    "            'To Bank': 'to_bank',\n",
    "            'Amount Received': 'amount_received',\n",
    "            'Receiving Currency': 'receiving_currency',\n",
    "            'Amount Paid': 'amount_paid',\n",
    "            'Payment Currency': 'payment_currency',\n",
    "            'Payment Format': 'payment_format',\n",
    "            'Is Laundering': 'is_laundering',\n",
    "        }\n",
    "    )\n",
    "    if 'Account' in df.columns:\n",
    "        df = df.rename(columns={'Account': 'from_account'})\n",
    "    if 'Account.1' in df.columns:\n",
    "        df = df.rename(columns={'Account.1': 'to_account'})\n",
    "    return df\n",
    "\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    work = df.copy()\n",
    "    work['timestamp'] = pd.to_datetime(work['Timestamp'], errors='coerce')\n",
    "\n",
    "    work['hour'] = work['timestamp'].dt.hour.fillna(-1).astype(int)\n",
    "    work['dayofweek'] = work['timestamp'].dt.dayofweek.fillna(-1).astype(int)\n",
    "    work['month'] = work['timestamp'].dt.month.fillna(-1).astype(int)\n",
    "    work['is_weekend'] = (work['dayofweek'] >= 5).astype(int)\n",
    "    work['same_bank'] = (work['from_bank'] == work['to_bank']).astype(int)\n",
    "    if 'from_account' in work.columns and 'to_account' in work.columns:\n",
    "        work['same_account'] = (work['from_account'] == work['to_account']).astype(int)\n",
    "    else:\n",
    "        work['same_account'] = 0\n",
    "    work['amount_diff'] = work['amount_received'] - work['amount_paid']\n",
    "    work['amount_ratio'] = np.divide(\n",
    "        work['amount_received'],\n",
    "        work['amount_paid'],\n",
    "        out=np.full(work.shape[0], np.nan, dtype=float),\n",
    "        where=work['amount_paid'].abs() > 0,\n",
    "    )\n",
    "    work['amount_ratio'] = np.where(np.isfinite(work['amount_ratio']), work['amount_ratio'], np.nan)\n",
    "    work['is_round_amount'] = ((work['amount_paid'] % 100) == 0).astype(int)\n",
    "\n",
    "    feature_cols = [\n",
    "        'amount_received',\n",
    "        'amount_paid',\n",
    "        'amount_diff',\n",
    "        'amount_ratio',\n",
    "        'hour',\n",
    "        'dayofweek',\n",
    "        'month',\n",
    "        'is_weekend',\n",
    "        'same_bank',\n",
    "        'same_account',\n",
    "        'is_round_amount',\n",
    "        'from_bank',\n",
    "        'to_bank',\n",
    "        'receiving_currency',\n",
    "        'payment_currency',\n",
    "        'payment_format',\n",
    "    ]\n",
    "    return work[feature_cols], work['is_laundering'].astype(int)\n",
    "\n",
    "\n",
    "def split_data(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    test_size: float,\n",
    "    val_size: float,\n",
    "    random_state: int,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n",
    "    X_tmp, X_test, y_tmp, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    relative_val = val_size / (1.0 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_tmp,\n",
    "        y_tmp,\n",
    "        test_size=relative_val,\n",
    "        stratify=y_tmp,\n",
    "        random_state=random_state + 1,\n",
    "    )\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def prepare_categorical(\n",
    "    X_train: pd.DataFrame,\n",
    "    X_val: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    categorical_cols: List[str],\n",
    ") -> Tuple[Dict[str, Dict[str, int]], Dict[str, int], Dict[str, np.ndarray], Dict[str, np.ndarray], Dict[str, np.ndarray]]:\n",
    "    mappings: Dict[str, Dict[str, int]] = {}\n",
    "    cardinalities: Dict[str, int] = {}\n",
    "    train_encoded: Dict[str, np.ndarray] = {}\n",
    "    val_encoded: Dict[str, np.ndarray] = {}\n",
    "    test_encoded: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        uniques = X_train[col].dropna().unique().tolist()\n",
    "        mapping = {value: idx + 1 for idx, value in enumerate(uniques)}\n",
    "        mappings[col] = mapping\n",
    "        cardinalities[col] = len(mapping) + 1\n",
    "\n",
    "        def encode(series: pd.Series) -> np.ndarray:\n",
    "            coded = series.map(mapping).fillna(0).astype(np.int64)\n",
    "            return coded.to_numpy()\n",
    "\n",
    "        train_encoded[col] = encode(X_train[col])\n",
    "        val_encoded[col] = encode(X_val[col])\n",
    "        test_encoded[col] = encode(X_test[col])\n",
    "\n",
    "    return mappings, cardinalities, train_encoded, val_encoded, test_encoded\n",
    "\n",
    "\n",
    "def prepare_numeric(\n",
    "    X_train: pd.DataFrame,\n",
    "    X_val: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    numeric_cols: List[str],\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    train_numeric = X_train[numeric_cols].copy()\n",
    "    medians = train_numeric.median()\n",
    "    train_numeric = train_numeric.fillna(medians)\n",
    "    means = train_numeric.mean()\n",
    "    stds = train_numeric.std().replace(0, 1.0)\n",
    "\n",
    "    def transform(df: pd.DataFrame) -> np.ndarray:\n",
    "        filled = df[numeric_cols].fillna(medians)\n",
    "        normalized = (filled - means) / stds\n",
    "        return normalized.to_numpy(dtype=np.float32)\n",
    "\n",
    "    train_array = transform(X_train)\n",
    "    val_array = transform(X_val)\n",
    "    test_array = transform(X_test)\n",
    "    return train_array, val_array, test_array\n",
    "\n",
    "\n",
    "def build_datasets(\n",
    "    X_train: pd.DataFrame,\n",
    "    X_val: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    y_val: pd.Series,\n",
    "    y_test: pd.Series,\n",
    "    numeric_cols: List[str],\n",
    "    categorical_cols: List[str],\n",
    ") -> Tuple[\n",
    "    TabularDataset,\n",
    "    TabularDataset,\n",
    "    TabularDataset,\n",
    "    List[int],\n",
    "]:\n",
    "    _, cardinalities, train_cats, val_cats, test_cats = prepare_categorical(\n",
    "        X_train, X_val, X_test, categorical_cols\n",
    "    )\n",
    "    train_numeric, val_numeric, test_numeric = prepare_numeric(\n",
    "        X_train, X_val, X_test, numeric_cols\n",
    "    )\n",
    "\n",
    "    def stack_cats(encoded: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        if not encoded:\n",
    "            return np.zeros((len(y_train), 0), dtype=np.int64)\n",
    "        ordered = [encoded[col] for col in categorical_cols]\n",
    "        return np.stack(ordered, axis=1)\n",
    "\n",
    "    train_cat = stack_cats(train_cats)\n",
    "    val_cat = stack_cats(val_cats)\n",
    "    test_cat = stack_cats(test_cats)\n",
    "\n",
    "    train_ds = TabularDataset(train_numeric, train_cat, y_train.to_numpy())\n",
    "    val_ds = TabularDataset(val_numeric, val_cat, y_val.to_numpy())\n",
    "    test_ds = TabularDataset(test_numeric, test_cat, y_test.to_numpy())\n",
    "\n",
    "    cardinality_list = [cardinalities[col] for col in categorical_cols]\n",
    "    return train_ds, val_ds, test_ds, cardinality_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c07b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_device(requested: str) -> torch.device:\n",
    "    if requested == 'auto':\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.device('cuda')\n",
    "        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "            return torch.device('mps')\n",
    "        return torch.device('cpu')\n",
    "    return torch.device(requested)\n",
    "\n",
    "\n",
    "def create_model(\n",
    "    train_dataset: TabularDataset,\n",
    "    cardinalities: List[int],\n",
    "    hidden_dims: List[int],\n",
    "    dropout: float,\n",
    "    base_embed_dim: int,\n",
    "    max_embed_dim: int,\n",
    ") -> TabularMLP:\n",
    "    num_numeric = train_dataset.numeric.shape[1]\n",
    "    model = TabularMLP(\n",
    "        num_numeric=num_numeric,\n",
    "        categorical_cardinalities=cardinalities,\n",
    "        hidden_dims=hidden_dims,\n",
    "        dropout=dropout,\n",
    "        base_embed_dim=base_embed_dim,\n",
    "        max_embed_dim=max_embed_dim,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def threshold_metrics(\n",
    "    y_true: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    threshold: float,\n",
    ") -> Dict[str, Any]:\n",
    "    preds = (y_prob >= threshold).astype(int)\n",
    "    tp = int(np.sum((preds == 1) & (y_true == 1)))\n",
    "    fp = int(np.sum((preds == 1) & (y_true == 0)))\n",
    "    fn = int(np.sum((preds == 0) & (y_true == 1)))\n",
    "    tn = int(np.sum((preds == 0) & (y_true == 0)))\n",
    "    precision = tp / (tp + fp) if tp + fp else 0.0\n",
    "    recall = tp / (tp + fn) if tp + fn else 0.0\n",
    "    fpr = fp / (fp + tn) if fp + tn else 0.0\n",
    "    tnr = tn / (tn + fp) if tn + fp else 0.0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if precision + recall else 0.0\n",
    "    return {\n",
    "        'threshold': threshold,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'fpr': fpr,\n",
    "        'tnr': tnr,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'tn': tn,\n",
    "    }\n",
    "\n",
    "\n",
    "def select_thresholds(\n",
    "    y_true: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    target_fpr: float | None,\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "    results: Dict[str, Dict[str, Any]] = {}\n",
    "    results['default'] = threshold_metrics(y_true, y_prob, 0.5)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    thresholds_extended = np.append(thresholds, 1.0)\n",
    "    denom = precision + recall\n",
    "    f1_scores = np.divide(\n",
    "        2 * precision * recall,\n",
    "        denom,\n",
    "        out=np.zeros_like(denom),\n",
    "        where=denom > 0,\n",
    "    )\n",
    "    best_idx = int(f1_scores.argmax())\n",
    "    best_threshold = float(thresholds_extended[best_idx])\n",
    "    results['best_f1'] = threshold_metrics(y_true, y_prob, best_threshold)\n",
    "\n",
    "    if target_fpr is not None:\n",
    "        grid = np.linspace(0.0, 1.0, num=501)\n",
    "        viable: List[Dict[str, Any]] = []\n",
    "        for candidate in grid:\n",
    "            metrics = threshold_metrics(y_true, y_prob, float(candidate))\n",
    "            if metrics['fpr'] <= target_fpr:\n",
    "                viable.append(metrics)\n",
    "        if viable:\n",
    "            results['target_fpr'] = max(viable, key=lambda item: item['recall'])\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: Iterable,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    criterion: nn.Module,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    for numeric, categorical, labels in loader:\n",
    "        numeric = numeric.to(device)\n",
    "        categorical = categorical.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(numeric, categorical)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        total_loss += loss.detach().item() * batch_size\n",
    "        total_samples += batch_size\n",
    "    return total_loss / max(total_samples, 1)\n",
    "\n",
    "\n",
    "def predict_proba(\n",
    "    model: nn.Module,\n",
    "    loader: Iterable,\n",
    "    device: torch.device,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    model.eval()\n",
    "    all_probs: List[np.ndarray] = []\n",
    "    all_labels: List[np.ndarray] = []\n",
    "    with torch.no_grad():\n",
    "        for numeric, categorical, labels in loader:\n",
    "            numeric = numeric.to(device)\n",
    "            categorical = categorical.to(device)\n",
    "            logits = model(numeric, categorical)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_labels.append(labels.numpy())\n",
    "    return np.concatenate(all_probs), np.concatenate(all_labels)\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    loader: Iterable,\n",
    "    device: torch.device,\n",
    "    target_fpr: float | None,\n",
    ") -> Dict[str, Any]:\n",
    "    probs, labels = predict_proba(model, loader, device)\n",
    "    roc_auc = roc_auc_score(labels, probs)\n",
    "    precision, recall, _ = precision_recall_curve(labels, probs)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    thresholds = select_thresholds(labels, probs, target_fpr)\n",
    "    return {\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'thresholds': thresholds,\n",
    "        'positive_rate': float(labels.mean()),\n",
    "    }\n",
    "\n",
    "\n",
    "def format_hidden_dims(spec: str) -> List[int]:\n",
    "    if not spec:\n",
    "        return []\n",
    "    return [int(part.strip()) for part in spec.split(',') if part.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d19d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss 0.7055 | Val ROC AUC 0.9484 | Val PR AUC 0.0280\n",
      "Epoch 002 | Loss 0.4709 | Val ROC AUC 0.9436 | Val PR AUC 0.0470\n",
      "Epoch 002 | Loss 0.4709 | Val ROC AUC 0.9436 | Val PR AUC 0.0470\n",
      "Epoch 003 | Loss 0.3882 | Val ROC AUC 0.9335 | Val PR AUC 0.0641\n",
      "Epoch 003 | Loss 0.3882 | Val ROC AUC 0.9335 | Val PR AUC 0.0641\n",
      "Epoch 004 | Loss 0.3443 | Val ROC AUC 0.9321 | Val PR AUC 0.0807\n",
      "Epoch 004 | Loss 0.3443 | Val ROC AUC 0.9321 | Val PR AUC 0.0807\n",
      "Epoch 005 | Loss 0.3195 | Val ROC AUC 0.9070 | Val PR AUC 0.0652\n",
      "Epoch 005 | Loss 0.3195 | Val ROC AUC 0.9070 | Val PR AUC 0.0652\n",
      "Epoch 006 | Loss 0.2908 | Val ROC AUC 0.9255 | Val PR AUC 0.1010\n",
      "Epoch 006 | Loss 0.2908 | Val ROC AUC 0.9255 | Val PR AUC 0.1010\n",
      "Epoch 007 | Loss 0.2634 | Val ROC AUC 0.8877 | Val PR AUC 0.0998\n",
      "Epoch 007 | Loss 0.2634 | Val ROC AUC 0.8877 | Val PR AUC 0.0998\n",
      "Epoch 008 | Loss 0.2486 | Val ROC AUC 0.9121 | Val PR AUC 0.1201\n",
      "Epoch 008 | Loss 0.2486 | Val ROC AUC 0.9121 | Val PR AUC 0.1201\n",
      "Epoch 009 | Loss 0.2328 | Val ROC AUC 0.9025 | Val PR AUC 0.0898\n",
      "Epoch 009 | Loss 0.2328 | Val ROC AUC 0.9025 | Val PR AUC 0.0898\n",
      "Epoch 010 | Loss 0.2249 | Val ROC AUC 0.8774 | Val PR AUC 0.0965\n",
      "Epoch 010 | Loss 0.2249 | Val ROC AUC 0.8774 | Val PR AUC 0.0965\n",
      "Epoch 011 | Loss 0.2037 | Val ROC AUC 0.8551 | Val PR AUC 0.1149\n",
      "Epoch 011 | Loss 0.2037 | Val ROC AUC 0.8551 | Val PR AUC 0.1149\n",
      "Epoch 012 | Loss 0.1950 | Val ROC AUC 0.8942 | Val PR AUC 0.1111\n",
      "Epoch 012 | Loss 0.1950 | Val ROC AUC 0.8942 | Val PR AUC 0.1111\n",
      "Epoch 013 | Loss 0.1838 | Val ROC AUC 0.8981 | Val PR AUC 0.1220\n",
      "Epoch 013 | Loss 0.1838 | Val ROC AUC 0.8981 | Val PR AUC 0.1220\n",
      "Epoch 014 | Loss 0.1771 | Val ROC AUC 0.8791 | Val PR AUC 0.1067\n",
      "Epoch 014 | Loss 0.1771 | Val ROC AUC 0.8791 | Val PR AUC 0.1067\n",
      "Epoch 015 | Loss 0.1660 | Val ROC AUC 0.8745 | Val PR AUC 0.1048\n",
      "Epoch 015 | Loss 0.1660 | Val ROC AUC 0.8745 | Val PR AUC 0.1048\n",
      "Epoch 016 | Loss 0.1532 | Val ROC AUC 0.8427 | Val PR AUC 0.1189\n",
      "Epoch 016 | Loss 0.1532 | Val ROC AUC 0.8427 | Val PR AUC 0.1189\n",
      "Epoch 017 | Loss 0.1573 | Val ROC AUC 0.8575 | Val PR AUC 0.1204\n",
      "Epoch 017 | Loss 0.1573 | Val ROC AUC 0.8575 | Val PR AUC 0.1204\n",
      "Epoch 018 | Loss 0.1502 | Val ROC AUC 0.8759 | Val PR AUC 0.1020\n",
      "Epoch 018 | Loss 0.1502 | Val ROC AUC 0.8759 | Val PR AUC 0.1020\n",
      "Epoch 019 | Loss 0.1457 | Val ROC AUC 0.8614 | Val PR AUC 0.1202\n",
      "Epoch 019 | Loss 0.1457 | Val ROC AUC 0.8614 | Val PR AUC 0.1202\n",
      "Epoch 020 | Loss 0.1371 | Val ROC AUC 0.8427 | Val PR AUC 0.1085\n",
      "Epoch 020 | Loss 0.1371 | Val ROC AUC 0.8427 | Val PR AUC 0.1085\n",
      "\n",
      "=== Validation Metrics ===\n",
      "ROC AUC: 0.8981\n",
      "PR AUC : 0.1220\n",
      "Positive prevalence: 0.1020%\n",
      "\n",
      "Threshold strategy: default\n",
      "  threshold : 0.5000\n",
      "  precision : 0.0096\n",
      "  recall    : 0.6274\n",
      "  f1        : 0.0189\n",
      "  fpr       : 0.0662\n",
      "  tnr       : 0.9338\n",
      "  TP : 325\n",
      "  FP : 33574\n",
      "  FN : 193\n",
      "  TN : 473743\n",
      "\n",
      "Threshold strategy: best_f1\n",
      "  threshold : 0.9986\n",
      "  precision : 0.2562\n",
      "  recall    : 0.2201\n",
      "  f1        : 0.2368\n",
      "  fpr       : 0.0007\n",
      "  tnr       : 0.9993\n",
      "  TP : 114\n",
      "  FP : 331\n",
      "  FN : 404\n",
      "  TN : 506986\n",
      "\n",
      "Threshold strategy: target_fpr\n",
      "  threshold : 0.6740\n",
      "  precision : 0.0118\n",
      "  recall    : 0.5849\n",
      "  f1        : 0.0232\n",
      "  fpr       : 0.0498\n",
      "  tnr       : 0.9502\n",
      "  TP : 303\n",
      "  FP : 25282\n",
      "  FN : 215\n",
      "  TN : 482035\n",
      "\n",
      "=== Test Metrics ===\n",
      "ROC AUC: 0.9148\n",
      "PR AUC : 0.1056\n",
      "Positive prevalence: 0.1019%\n",
      "\n",
      "Threshold strategy: default\n",
      "  threshold : 0.5000\n",
      "  precision : 0.0100\n",
      "  recall    : 0.6580\n",
      "  f1        : 0.0197\n",
      "  fpr       : 0.0663\n",
      "  tnr       : 0.9337\n",
      "  TP : 681\n",
      "  FP : 67257\n",
      "  FN : 354\n",
      "  TN : 947377\n",
      "\n",
      "Threshold strategy: best_f1\n",
      "  threshold : 0.9988\n",
      "  precision : 0.2589\n",
      "  recall    : 0.1903\n",
      "  f1        : 0.2194\n",
      "  fpr       : 0.0006\n",
      "  tnr       : 0.9994\n",
      "  TP : 197\n",
      "  FP : 564\n",
      "  FN : 838\n",
      "  TN : 1014070\n",
      "\n",
      "Threshold strategy: target_fpr\n",
      "  threshold : 0.6720\n",
      "  precision : 0.0124\n",
      "  recall    : 0.6145\n",
      "  f1        : 0.0243\n",
      "  fpr       : 0.0499\n",
      "  tnr       : 0.9501\n",
      "  TP : 636\n",
      "  FP : 50675\n",
      "  FN : 399\n",
      "  TN : 963959\n",
      "\n",
      "=== Validation Metrics ===\n",
      "ROC AUC: 0.8981\n",
      "PR AUC : 0.1220\n",
      "Positive prevalence: 0.1020%\n",
      "\n",
      "Threshold strategy: default\n",
      "  threshold : 0.5000\n",
      "  precision : 0.0096\n",
      "  recall    : 0.6274\n",
      "  f1        : 0.0189\n",
      "  fpr       : 0.0662\n",
      "  tnr       : 0.9338\n",
      "  TP : 325\n",
      "  FP : 33574\n",
      "  FN : 193\n",
      "  TN : 473743\n",
      "\n",
      "Threshold strategy: best_f1\n",
      "  threshold : 0.9986\n",
      "  precision : 0.2562\n",
      "  recall    : 0.2201\n",
      "  f1        : 0.2368\n",
      "  fpr       : 0.0007\n",
      "  tnr       : 0.9993\n",
      "  TP : 114\n",
      "  FP : 331\n",
      "  FN : 404\n",
      "  TN : 506986\n",
      "\n",
      "Threshold strategy: target_fpr\n",
      "  threshold : 0.6740\n",
      "  precision : 0.0118\n",
      "  recall    : 0.5849\n",
      "  f1        : 0.0232\n",
      "  fpr       : 0.0498\n",
      "  tnr       : 0.9502\n",
      "  TP : 303\n",
      "  FP : 25282\n",
      "  FN : 215\n",
      "  TN : 482035\n",
      "\n",
      "=== Test Metrics ===\n",
      "ROC AUC: 0.9148\n",
      "PR AUC : 0.1056\n",
      "Positive prevalence: 0.1019%\n",
      "\n",
      "Threshold strategy: default\n",
      "  threshold : 0.5000\n",
      "  precision : 0.0100\n",
      "  recall    : 0.6580\n",
      "  f1        : 0.0197\n",
      "  fpr       : 0.0663\n",
      "  tnr       : 0.9337\n",
      "  TP : 681\n",
      "  FP : 67257\n",
      "  FN : 354\n",
      "  TN : 947377\n",
      "\n",
      "Threshold strategy: best_f1\n",
      "  threshold : 0.9988\n",
      "  precision : 0.2589\n",
      "  recall    : 0.1903\n",
      "  f1        : 0.2194\n",
      "  fpr       : 0.0006\n",
      "  tnr       : 0.9994\n",
      "  TP : 197\n",
      "  FP : 564\n",
      "  FN : 838\n",
      "  TN : 1014070\n",
      "\n",
      "Threshold strategy: target_fpr\n",
      "  threshold : 0.6720\n",
      "  precision : 0.0124\n",
      "  recall    : 0.6145\n",
      "  f1        : 0.0243\n",
      "  fpr       : 0.0499\n",
      "  tnr       : 0.9501\n",
      "  TP : 636\n",
      "  FP : 50675\n",
      "  FN : 399\n",
      "  TN : 963959\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(cfg.random_state)\n",
    "np.random.seed(cfg.random_state)\n",
    "\n",
    "df = load_transactions(Path(cfg.dataset), cfg.max_samples)\n",
    "X, y = engineer_features(df)\n",
    "\n",
    "numeric_cols = [\n",
    "    \"amount_received\",\n",
    "    \"amount_paid\",\n",
    "    \"amount_diff\",\n",
    "    \"amount_ratio\",\n",
    "    \"hour\",\n",
    "    \"dayofweek\",\n",
    "    \"month\",\n",
    "    \"is_weekend\",\n",
    "    \"same_bank\",\n",
    "    \"same_account\",\n",
    "    \"is_round_amount\",\n",
    "]\n",
    "categorical_cols = [\n",
    "    \"from_bank\",\n",
    "    \"to_bank\",\n",
    "    \"receiving_currency\",\n",
    "    \"payment_currency\",\n",
    "    \"payment_format\",\n",
    "]\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=cfg.test_size,\n",
    "    val_size=cfg.val_size,\n",
    "    random_state=cfg.random_state,\n",
    "    )\n",
    "\n",
    "train_ds, val_ds, test_ds, cardinalities = build_datasets(\n",
    "    X_train,\n",
    "    X_val,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_val,\n",
    "    y_test,\n",
    "    numeric_cols,\n",
    "    categorical_cols,\n",
    "    )\n",
    "\n",
    "hidden_dims = format_hidden_dims(cfg.hidden_dims)\n",
    "model = create_model(\n",
    "    train_ds,\n",
    "    cardinalities,\n",
    "    hidden_dims,\n",
    "    cfg.dropout,\n",
    "    cfg.base_embed_dim,\n",
    "    cfg.max_embed_dim,\n",
    "    )\n",
    "\n",
    "device = determine_device(cfg.device)\n",
    "model.to(device)\n",
    "pin = device.type == \"cuda\"\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=pin,\n",
    "    )\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=pin,\n",
    "    )\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=pin,\n",
    "    )\n",
    "\n",
    "positives = float(y_train.sum())\n",
    "negatives = float(len(y_train) - positives)\n",
    "pos_weight_value = negatives / max(positives, 1.0)\n",
    "pos_weight = torch.tensor(pos_weight_value, dtype=torch.float32, device=device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay\n",
    ")\n",
    "\n",
    "best_state = deepcopy(model.state_dict())\n",
    "best_metric = -np.inf\n",
    "\n",
    "for epoch in range(1, cfg.epochs + 1):\n",
    "    train_loss = run_epoch(model, train_loader, optimizer, device, criterion)\n",
    "    val_metrics = evaluate(model, val_loader, device, cfg.target_fpr)\n",
    "    if val_metrics[\"pr_auc\"] > best_metric:\n",
    "        best_metric = val_metrics[\"pr_auc\"]\n",
    "        best_state = deepcopy(model.state_dict())\n",
    "    print(\n",
    "        f\"Epoch {epoch:03d} | Loss {train_loss:.4f} | Val ROC AUC {val_metrics['roc_auc']:.4f} | Val PR AUC {val_metrics['pr_auc']:.4f}\"\n",
    "    )\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "val_metrics = evaluate(model, val_loader, device, cfg.target_fpr)\n",
    "test_metrics = evaluate(model, test_loader, device, cfg.target_fpr)\n",
    "\n",
    "print(\"\\n=== Validation Metrics ===\")\n",
    "print(f\"ROC AUC: {val_metrics['roc_auc']:.4f}\")\n",
    "print(f\"PR AUC : {val_metrics['pr_auc']:.4f}\")\n",
    "print(f\"Positive prevalence: {val_metrics['positive_rate']:.4%}\")\n",
    "for name, detail in val_metrics[\"thresholds\"].items():\n",
    "    print(f\"\\nThreshold strategy: {name}\")\n",
    "    for key, value in detail.items():\n",
    "        if key in {\"tp\", \"fp\", \"fn\", \"tn\"}:\n",
    "            print(f\"  {key.upper():<3}: {value}\")\n",
    "        else:\n",
    "            print(f\"  {key:<10}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n=== Test Metrics ===\")\n",
    "print(f\"ROC AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"PR AUC : {test_metrics['pr_auc']:.4f}\")\n",
    "print(f\"Positive prevalence: {test_metrics['positive_rate']:.4%}\")\n",
    "for name, detail in test_metrics[\"thresholds\"].items():\n",
    "    print(f\"\\nThreshold strategy: {name}\")\n",
    "    for key, value in detail.items():\n",
    "        if key in {\"tp\", \"fp\", \"fn\", \"tn\"}:\n",
    "            print(f\"  {key.upper():<3}: {value}\")\n",
    "        else:\n",
    "            print(f\"  {key:<10}: {value:.4f}\")\n",
    "\n",
    "if cfg.report:\n",
    "    report_path = Path(cfg.report)\n",
    "    report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    payload = {\n",
    "        \"validation\": val_metrics,\n",
    "        \"test\": test_metrics,\n",
    "    }\n",
    "    with report_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
    "        json.dump(payload, handle, indent=2)\n",
    "    print(f\"\\nMetrics saved to {report_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc865-anti-money-laundering-ibm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
